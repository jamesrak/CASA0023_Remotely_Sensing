[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Remotely Sensing Cities and Environments",
    "section": "",
    "text": "This is a learning diary of the module CASA0023: Remotely Sensing Cities and Environments offered by UCL. More details: https://andrewmaclachlan.github.io/CASA0023/\n\n\n\n\nJames - Nutthaphol Rakratchatakul\nStudent ID: 21174510\nMsc Urban Spatial Science\n\n\n\n\n\n\n\n\nJames’ research interest includes agricultural intelligence for sustainable planning, including combining satellite imagery and machine learning to help them monitor crop health, suggest how to optimize their resources and warn them before the disaster is coming.\n\n\n\n\nMSc., Urban Spatial Science, University College London (Present)  B.E., Computer Engineering, Chulalongkorn University (2018)"
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  SAR Sensor",
    "section": "",
    "text": "Slide: URL"
  },
  {
    "objectID": "week2.html#characteristics",
    "href": "week2.html#characteristics",
    "title": "2  SAR Sensor",
    "section": "2.2 Characteristics",
    "text": "2.2 Characteristics\n\n30m spatial resolution\nrevisit period is 16 days\n\nhttps://andrewmaclachlan.github.io/CASA0023-lecture-1/#1"
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "1  An Introduction to Remote Sensing",
    "section": "",
    "text": "There are two types of sensor which are active and passive sensors. Active sensors, such as Human eye, camera, satellite sensor, Use energy that is available and usually detect reflected energy from the sun. On the other hand, active sensors, such as Radar, X-ray and LiDAR, have an energy source for illumination and actively emit electromagnetic waves and then wait to receive.\n\n\nMost common data format is raster. Common file types include BIL, BSQ, BIP and GeoTIFF.\nResolutions of data are considered in many aspects, including spatial, spectral, temporal and radiometric. The example of resolutions is provided below:\n\n\n\n\n\n\n\n\nResolution\nExample\n\n\n\n\nSpatial\n30 m\n\n\nSpectral\nBand 4 - Red, Band 5 - Near Infrared (NIR), Band 6 - SWIR 1\n\n\nTemporal\ndaily\n\n\nRadiometric\nan 8 bit sensor has values between 0 and 255 (256 possible values)"
  },
  {
    "objectID": "SAR_presentation.html",
    "href": "SAR_presentation.html",
    "title": "Synthetic Aperture Radar (SAR)",
    "section": "",
    "text": "SAR Applications\n\n\n\n\n\n\n\n\n\nhttps://www.l3harrisgeospatial.com/Learn/Blogs/Blog-Details/ArtMID/10198/ArticleID/24031/Enhancing-Situational-Awareness-with-SAR-Data\n\n\n\nReflection"
  },
  {
    "objectID": "sample_xaringan_presentation.html",
    "href": "sample_xaringan_presentation.html",
    "title": "Presentation Ninja",
    "section": "",
    "text": "???\nImage credit: Wikimedia Commons\nclass: inverse, center, middle\n\nGet Started\n\n\n\nHello World\nInstall the xaringan package from Github:\n\nremotes::install_github(\"yihui/xaringan\")\n\n–\nYou are recommended to use the RStudio IDE, but you do not have to.\n\nCreate a new R Markdown document from the menu File -> New File -> R Markdown -> From Template -> Ninja Presentation;1\n\n–\n\nClick the Knit button to compile it;\n\n–\n\nor use the RStudio Addin2 “Infinite Moon Reader” to live preview the slides (every time you update and save the Rmd document, the slides will be automatically reloaded in RStudio Viewer.\n\n.footnote[ [1] 中文用户请看这份教程\n[2] See #2 if you do not see the template or addin in RStudio. ]\n\n\nHello Ninja\nAs a presentation ninja, you certainly should not be satisfied by the “Hello World” example. You need to understand more about two things:\n\nThe remark.js library;\nThe xaringan package;\n\nBasically xaringan injected the chakra of R Markdown (minus Pandoc) into remark.js. The slides are rendered by remark.js in the web browser, and the Markdown source needed by remark.js is generated from R Markdown (knitr).\n\n\n\nremark.js\nYou can see an introduction of remark.js from its homepage. You should read the remark.js Wiki at least once to know how to\n\ncreate a new slide (Markdown syntax* and slide properties);\nformat a slide (e.g. text alignment);\nconfigure the slideshow;\nand use the presentation (keyboard shortcuts).\n\nIt is important to be familiar with remark.js before you can understand the options in xaringan.\n.footnote[[*] It is different with Pandoc’s Markdown! It is limited but should be enough for presentation purposes. Come on… You do not need a slide for the Table of Contents! Well, the Markdown support in remark.js may be improved in the future.]\nclass: inverse, middle, center\n\n\nUsing xaringan\n\n\n\nxaringan\nProvides an R Markdown output format xaringan::moon_reader as a wrapper for remark.js, and you can use it in the YAML metadata, e.g.\n---\ntitle: \"A Cool Presentation\"\noutput:\n  xaringan::moon_reader:\n    yolo: true\n    nature:\n      autoplay: 30000\n---\nSee the help page ?xaringan::moon_reader for all possible options that you can use.\n\n\n\nremark.js vs xaringan\nSome differences between using remark.js (left) and using xaringan (right):\n.pull-left[ 1. Start with a boilerplate HTML file;\n\nPlain Markdown;\nWrite JavaScript to autoplay slides;\nManually configure MathJax;\nHighlight code with *;\nEdit Markdown source and refresh browser to see updated slides; ]\n\n.pull-right[ 1. Start with an R Markdown document;\n\nR Markdown (can embed R/other code chunks);\nProvide an option autoplay;\nMathJax just works;*\nHighlight code with {{}};\nThe RStudio addin “Infinite Moon Reader” automatically refreshes slides on changes; ]\n\n.footnote[[*] Not really. See next page.]\n\n\n\nMath Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $+$ renders \\(\\alpha+\\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $.\nMath does not work on the title slide (see #61 for a workaround).\n\n\n\n\nR Code\n\n# a boring regression\nfit = lm(dist ~ 1 + speed, data = cars)\ncoef(summary(fit))\n\n#               Estimate Std. Error   t value     Pr(>|t|)\n# (Intercept) -17.579095  6.7584402 -2.601058 1.231882e-02\n# speed         3.932409  0.4155128  9.463990 1.489836e-12\n\ndojutsu = c('地爆天星', '天照', '加具土命', '神威', '須佐能乎', '無限月読')\ngrep('天', dojutsu, value = TRUE)\n\n# [1] \"地爆天星\" \"天照\"\n\n\n\n\n\nR Plots\n\npar(mar = c(4, 4, 1, .1))\nplot(cars, pch = 19, col = 'darkgray', las = 1)\nabline(fit, lwd = 2)\n\n\n\n\n\n\n\nTables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\nknitr::kable(head(iris), format = 'html')\n\n\n\n \n  \n    Sepal.Length \n    Sepal.Width \n    Petal.Length \n    Petal.Width \n    Species \n  \n \n\n  \n    5.1 \n    3.5 \n    1.4 \n    0.2 \n    setosa \n  \n  \n    4.9 \n    3.0 \n    1.4 \n    0.2 \n    setosa \n  \n  \n    4.7 \n    3.2 \n    1.3 \n    0.2 \n    setosa \n  \n  \n    4.6 \n    3.1 \n    1.5 \n    0.2 \n    setosa \n  \n  \n    5.0 \n    3.6 \n    1.4 \n    0.2 \n    setosa \n  \n  \n    5.4 \n    3.9 \n    1.7 \n    0.4 \n    setosa \n  \n\n\n\n\n\n\n\n\nHTML Widgets\nI have not thoroughly tested HTML widgets against xaringan. Some may work well, and some may not. It is a little tricky.\nSimilarly, the Shiny mode (runtime: shiny) does not work. I might get these issues fixed in the future, but these are not of high priority to me. I never turn my presentation into a Shiny app. When I need to demonstrate more complicated examples, I just launch them separately. It is convenient to share slides with other people when they are plain HTML/JS applications.\nSee the next page for two HTML widgets.\n\n\nlibrary(leaflet)\nleaflet() %>% addTiles() %>% setView(-93.65, 42.0285, zoom = 17)\n\n\n\n\n\n\n\nDT::datatable(\n  head(iris, 10),\n  fillContainer = FALSE, options = list(pageLength = 8)\n)\n\n\n\n\n\n\n\n\n\nSome Tips\n\nDo not forget to try the yolo option of xaringan::moon_reader.\noutput:\n  xaringan::moon_reader:\n    yolo: true\n\n\n\n\nSome Tips\n\nSlides can be automatically played if you set the autoplay option under nature, e.g. go to the next slide every 30 seconds in a lightning talk:\noutput:\n  xaringan::moon_reader:\n    nature:\n      autoplay: 30000\nIf you want to restart the play after it reaches the last slide, you may set the sub-option loop to TRUE, e.g.,\noutput:\n  xaringan::moon_reader:\n    nature:\n      autoplay:\n        interval: 30000\n        loop: true\n\n\n\n\nSome Tips\n\nA countdown timer can be added to every page of the slides using the countdown option under nature, e.g. if you want to spend one minute on every page when you give the talk, you can set:\noutput:\n  xaringan::moon_reader:\n    nature:\n      countdown: 60000\nThen you will see a timer counting down from 01:00, to 00:59, 00:58, … When the time is out, the timer will continue but the time turns red.\n\n\n\n\nSome Tips\n\nThe title slide is created automatically by xaringan, but it is just another remark.js slide added before your other slides.\nThe title slide is set to class: center, middle, inverse, title-slide by default. You can change the classes applied to the title slide with the titleSlideClass option of nature (title-slide is always applied).\noutput:\n  xaringan::moon_reader:\n    nature:\n      titleSlideClass: [top, left, inverse]\n\n–\n\nIf you’d like to create your own title slide, disable xaringan’s title slide with the seal = FALSE option of moon_reader.\noutput:\n  xaringan::moon_reader:\n    seal: false\n\n\n\n\nSome Tips\n\nThere are several ways to build incremental slides. See this presentation for examples.\nThe option highlightLines: true of nature will highlight code lines that start with *, or are wrapped in {{ }}, or have trailing comments #<<;\noutput:\n  xaringan::moon_reader:\n    nature:\n      highlightLines: true\nSee examples on the next page.\n\n\n\n\nSome Tips\n.pull-left[ An example using a leading *:\n```r\nif (TRUE) {\n** message(\"Very important!\")\n}\n```\nOutput:\nif (TRUE) {\n* message(\"Very important!\")\n}\nThis is invalid R code, so it is a plain fenced code block that is not executed. ]\n.pull-right[ An example using {{}}:\n```{r tidy=FALSE}\nif (TRUE) {\n*{{ message(\"Very important!\") }}\n}\n```\nOutput:\n\nif (TRUE) {\n{{ message(\"Very important!\") }}\n}\n\nVery important!\n\n\nIt is valid R code so you can run it. Note that {{}} can wrap an R expression of multiple lines. ]\n\n\n\nSome Tips\nAn example of using the trailing comment #<< to highlight lines:\n```{r tidy=FALSE}\nlibrary(ggplot2)\nggplot(mtcars) + \n  aes(mpg, disp) + \n  geom_point() +   #<<\n  geom_smooth()    #<<\n```\nOutput:\n\nlibrary(ggplot2)\nggplot(mtcars) + \n  aes(mpg, disp) + \n  geom_point() +   #<<\n  geom_smooth()    #<<\n\n\n\n\nSome Tips\nWhen you enable line-highlighting, you can also use the chunk option highlight.output to highlight specific lines of the text output from a code chunk. For example, highlight.output = TRUE means highlighting all lines, and highlight.output = c(1, 3) means highlighting the first and third line.\n```{r, highlight.output=c(1, 3)}\nhead(iris)\n```\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nQuestion: what does highlight.output = c(TRUE, FALSE) mean? (Hint: think about R’s recycling of vectors)\n\n\n\nSome Tips\n\nTo make slides work offline, you need to download a copy of remark.js in advance, because xaringan uses the online version by default (see the help page ?xaringan::moon_reader).\nYou can use xaringan::summon_remark() to download the latest or a specified version of remark.js. By default, it is downloaded to libs/remark-latest.min.js.\nThen change the chakra option in YAML to point to this file, e.g.\noutput:\n  xaringan::moon_reader:\n    chakra: libs/remark-latest.min.js\nIf you used Google fonts in slides (the default theme uses Yanone Kaffeesatz, Droid Serif, and Source Code Pro), they won’t work offline unless you download or install them locally. The Heroku app google-webfonts-helper can help you download fonts and generate the necessary CSS.\n\n\n\n\nMacros\n\nremark.js allows users to define custom macros (JS functions) that can be applied to Markdown text using the syntax ![:macroName arg1, arg2, ...] or ![:macroName arg1, arg2, ...](this). For example, before remark.js initializes the slides, you can define a macro named scale:\nremark.macros.scale = function (percentage) {\n  var url = this;\n  return '<img src=\"' + url + '\" style=\"width: ' + percentage + '\" />';\n};\nThen the Markdown text\n![:scale 50%](image.jpg)\nwill be translated to\n<img src=\"image.jpg\" style=\"width: 50%\" />\n\n\n\n\nMacros (continued)\n\nTo insert macros in xaringan slides, you can use the option beforeInit under the option nature, e.g.,\noutput:\n  xaringan::moon_reader:\n    nature:\n      beforeInit: \"macros.js\"\nYou save your remark.js macros in the file macros.js.\nThe beforeInit option can be used to insert arbitrary JS code before remark.create(). Inserting macros is just one of its possible applications.\n\n\n\n\nCSS\nAmong all options in xaringan::moon_reader, the most challenging but perhaps also the most rewarding one is css, because it allows you to customize the appearance of your slides using any CSS rules or hacks you know.\nYou can see the default CSS file here. You can completely replace it with your own CSS files, or define new rules to override the default. See the help page ?xaringan::moon_reader for more information.\n\n\n\nCSS\nFor example, suppose you want to change the font for code from the default “Source Code Pro” to “Ubuntu Mono”. You can create a CSS file named, say, ubuntu-mono.css:\n@import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);\n\n.remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }\nThen set the css option in the YAML metadata:\noutput:\n  xaringan::moon_reader:\n    css: [\"default\", \"ubuntu-mono.css\"]\nHere I assume ubuntu-mono.css is under the same directory as your Rmd.\nSee yihui/xaringan#83 for an example of using the Fira Code font, which supports ligatures in program code.\n\n\n\nCSS (with Sass)\nxaringan also supports Sass support via rmarkdown. Suppose you want to use the same color for different elements, e.g., first heading and bold text. You can create a .scss file, say mytheme.scss, using the sass syntax with variables:\n$mycolor: #ff0000; \n.remark-slide-content > h1 { color: $mycolor; }\n.remark-slide-content strong { color: $mycolor; }\nThen set the css option in the YAML metadata using this file placed under the same directory as your Rmd:\noutput:\n  xaringan::moon_reader:\n    css: [\"default\", \"mytheme.scss\"]\nThis requires rmarkdown >= 2.8 and the sass package. You can learn more about rmarkdown and sass support in this blog post and in sass overview vignette.\n\n\n\nThemes\nDon’t want to learn CSS? Okay, you can use some user-contributed themes. A theme typically consists of two CSS files foo.css and foo-fonts.css, where foo is the theme name. Below are some existing themes:\n\nnames(xaringan:::list_css())\n\n [1] \"chocolate-fonts\"  \"chocolate\"        \"default-fonts\"   \n [4] \"default\"          \"duke-blue\"        \"fc-fonts\"        \n [7] \"fc\"               \"glasgow_template\" \"hygge-duke\"      \n[10] \"hygge\"            \"ki-fonts\"         \"ki\"              \n[13] \"kunoichi\"         \"lucy-fonts\"       \"lucy\"            \n[16] \"metropolis-fonts\" \"metropolis\"       \"middlebury-fonts\"\n[19] \"middlebury\"       \"nhsr-fonts\"       \"nhsr\"            \n[22] \"ninjutsu\"         \"rladies-fonts\"    \"rladies\"         \n[25] \"robot-fonts\"      \"robot\"            \"rutgers-fonts\"   \n[28] \"rutgers\"          \"shinobi\"          \"tamu-fonts\"      \n[31] \"tamu\"             \"uio-fonts\"        \"uio\"             \n[34] \"uo-fonts\"         \"uo\"               \"uol-fonts\"       \n[37] \"uol\"              \"useR-fonts\"       \"useR\"            \n[40] \"uwm-fonts\"        \"uwm\"              \"wic-fonts\"       \n[43] \"wic\"             \n\n\n\n\n\nThemes\nTo use a theme, you can specify the css option as an array of CSS filenames (without the .css extensions), e.g.,\noutput:\n  xaringan::moon_reader:\n    css: [default, metropolis, metropolis-fonts]\nIf you want to contribute a theme to xaringan, please read this blog post.\nbackground-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg) background-size: 100px background-position: 90% 8%\n\n\nSharingan\nThe R package name xaringan was derived1 from Sharingan, a dōjutsu in the Japanese anime Naruto with two abilities:\n\nthe “Eye of Insight”\nthe “Eye of Hypnotism”\n\nI think a presentation is basically a way to communicate insights to the audience, and a great presentation may even “hypnotize” the audience.2,3\n.footnote[ [1] In Chinese, the pronounciation of X is Sh /ʃ/ (as in shrimp). Now you should have a better idea of how to pronounce my last name Xie.\n[2] By comparison, bad presentations only put the audience to sleep.\n[3] Personally I find that setting background images for slides is a killer feature of remark.js. It is an effective way to bring visual impact into your presentations. ]\n\n\n\nNaruto terminology\nThe xaringan package borrowed a few terms from Naruto, such as\n\nSharingan (写輪眼; the package name)\nThe moon reader (月読; an attractive R Markdown output format)\nChakra (查克拉; the path to the remark.js library, which is the power to drive the presentation)\nNature transformation (性質変化; transform the chakra by setting different options)\nThe infinite moon reader (無限月読; start a local web server to continuously serve your slides)\nThe summoning technique (download remark.js from the web)\n\nYou can click the links to know more about them if you want. The jutsu “Moon Reader” may seem a little evil, but that does not mean your slides are evil.\n\nclass: center\n\n\nHand seals (印)\nPress h or ? to see the possible ninjutsu you can use in remark.js.\n\n\nclass: center, middle\n\n\nThanks!\nSlides created via the R package xaringan.\nThe chakra comes from remark.js, knitr, and R Markdown."
  },
  {
    "objectID": "week2.html#additional-reading",
    "href": "week2.html#additional-reading",
    "title": "2  SAR Sensor",
    "section": "2.3 Additional Reading",
    "text": "2.3 Additional Reading\nRADAR VS. OPTICAL: OPTIMISING SATELLITE USE IN LAND COVER CLASSIFICATION https://ecologyforthemasses.com/2020/05/27/radar-vs-optical-optimising-satellite-use-in-land-cover-classification/\n An optical image of Kliuchevskoi volcano on the left, with a radar image on the right (Image credit: Michigan Tech Volcanology, Image Cropped)\n\nComparison of land cover classification results using a temporal average (left) and time series (right) of a combination of optical and radar images (Image Credit: Lopes et al., 2020)\nThe categorization accuracy was tested using the so-called Kappa-coefficient. According to their findings, classifications based on radar time series were more accurate than those based on optical time series. However, more precise findings may not always result in better maps. Therefore, while examining maps, the possibility of inaccuracies caused by the similarity of categories in optical or radar data should be considered."
  },
  {
    "objectID": "week2.html#radar-vs.-optical-optimising-satellite-use-in-land-cover-classification",
    "href": "week2.html#radar-vs.-optical-optimising-satellite-use-in-land-cover-classification",
    "title": "2  SAR Sensor",
    "section": "2.2 Radar VS. Optical: Optimising satellite use in land cover classification",
    "text": "2.2 Radar VS. Optical: Optimising satellite use in land cover classification\nAdditional reading: I found the interesting article comparing the results of using radar and optical images in land cover classification. Source: Julia Ramsauer 2020\n An optical image of Kliuchevskoi volcano on the left, with a radar image on the right (Image credit: Michigan Tech Volcanology, Image Cropped)\n\nComparison of land cover classification results using a temporal average (left) and time series (right) of a combination of optical and radar images (Image Credit: Lopes et al., 2020 )\nThe categorization accuracy was tested using the so-called Kappa-coefficient. According to their findings, classifications based on radar time series were more accurate than those based on optical time series. However, more precise findings may not always result in better maps. Therefore, while examining maps, the possibility of inaccuracies caused by the similarity of categories in optical or radar data should be considered."
  },
  {
    "objectID": "week2.html#synthetic-aperture-radar-sar",
    "href": "week2.html#synthetic-aperture-radar-sar",
    "title": "2  SAR Sensor",
    "section": "2.1 Synthetic Aperture Radar (SAR)",
    "text": "2.1 Synthetic Aperture Radar (SAR)\n\nUsing radar over optical remote sensing\nAdvantanges\n\n\n(+) Ability to capture data day or night\n\n\n(+) See through clouds\n\n\n(+) Weather independence by selecting proper frequency range\n\n\n(+) Penetration through the vegetation canopy and the soil\n\n\n(+/-) Sensitivity to structure\nDisadvantanges\n\n\n(-) Information content is different than optical and sometimes difficult to interpret\n\n\nVariable resolution is 1 to 100 m"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "https://ecologyforthemasses.com/2020/05/27/radar-vs-optical-optimising-satellite-use-in-land-cover-classification/\nhttps://www.l3harrisgeospatial.com/Learn/Blogs/Blog-Details/ArtMID/10198/ArticleID/24031/Enhancing-Situational-Awareness-with-SAR\nhttps://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar"
  },
  {
    "objectID": "week1.html#applications",
    "href": "week1.html#applications",
    "title": "1  An Introduction to Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nThere are many useful applications of remote sensing. Examples of agricultural applications include productivity increasing. This could be done by providing inputs, given from remote sensing data, for improvement of soil fertility. The other example is to identify culturable wastelands or marginal lands. The benefit of this application is to increase agricultural area (Navalgund, Jayaraman, and Roy 2007). Without remote sensing data, it will be difficult to retrieve data covering every area in the country.\nNext, let’s look into more detail of some research using remote sensing.\n\n\nSoil fertility assessment for optimal agricultural use using remote sensing and GIS technologies (AbdelRahman, Hegab, and Yossif 2021)\n\n\n\n\n\n\n\nSource: AbdelRahman, Hegab, and Yossif 2021\n\n\n\n\n\nSummary: This research aims to assess soil fertility for optimal agricultural use in Egypt using remote sensing. The study area is located in the north region of the west desert. The goal is to determine the suitable areas for different crops and irrigation systems. The results show that the high suitable areas for barley and wheat were about 54.7%, while for corn and beans were about 44.9%. To reduce water losses, the study recommends improving the delivery and efficiency of irrigation systems and choosing appropriate irrigation methods.\nMethodology: The researcher combined Sentinel-1 and DEM derived for conducting landform and soil mapping. Some indices related to soil were extracted from remote sensing data, such as soil fertility index (SFI) and soil evaluation factor (SEF). Ordinary kriging (OK) method was performed to predict unmeasured locations by incorporating spatial correlations. Soil capability was assessed by categorising into 3 classes from irrigation capability index.\nComments: This use case is quite impressive that they tried to use remote sensing data to help with soil assessment. It can be very tough if we do not have remote sensing data. However, this research mentioned that “Collected soil samples were analyzed according to USDA (2014). Soil taxonomy was produced using USDA (2014).” I wondered if the assessment of crops in Egypt can follow every guideline provided by the United States. The species of crops may different between country and have to adapt for some implementation."
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "1  An Introduction to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\n\nContent: As I used to work in the feed trading company before, remote sensing plays an important role of estimating crop supply. To identify the location of all crops such as corn and soybean in the country, we need to depend on remote sensing data to quickly estimate the supply of each crop. Otherwise, human are needed to survey in every fields, which is quite exhaustive. Furthermore, it is good to see the availability of data with different resolution, so we can choose and use it with a proper task.\nApplications: The selected research looks very useful and be likely to be able to apply for a better agricultural use. However, there are so many methods mentioned in this research, such as Ordinary kriging (OK) that I have not learned before. I expect that after studying this course for a while, I can have a better understanding of the remote sensing research more.\n\n\n\n\n\nAbdelRahman, Mohamed A. E., Rehab H. Hegab, and Taher M. H. Yossif. 2021. “Soil Fertility Assessment for Optimal Agricultural Use Using Remote Sensing and GIS Technologies.” Applied Geomatics 13 (4): 605–18. https://doi.org/10.1007/s12518-021-00376-1.\n\n\nNavalgund, Ranganath R., V. Jayaraman, and P. S. Roy. 2007. “Remote Sensing Applications: An Overview.” Current Science 93 (12): 1747–66. http://www.jstor.org/stable/24102069."
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "3  Remote sensing data",
    "section": "",
    "text": "In this week, we will focus on the techniques using for data pre-processing.\n\n\nRemote Sensing data are usually not ready to use, due to many reasons such as errors from sensor devices and cloud covering. Therefore, it is needed to correct before using. There are many corrections techniques as following:\n\n\n\nCorrection\nSolutions\n\n\n\n\n1. Geometric correctionSource:Abdul Basith\nCorrection Techniques: Backward mapping (Linear Regression), Resampling  Explanation: Remotely sensed image distortions can be happened due to view angle, topography or wind. Therefore, we may use GCP (Ground control points) with polynomial function to correct\n\n\n2. Atmospheric CorrectionSource:Liang et al. 2001\nCorrection Techniques: Dark object subtraction (DOS), Psuedo-invariant Features (PIFs) Explanation: Absorption and scattering may reduces contrast of image and cause adjacency effect, which is the situation when radiance from pixels nearby is mixed into pixel of interest. One of the solutions is to apply Dark object subtraction (DOS) by searching each band for the darkest value then subtracts that from each pixel. The other method is Psuedo-invariant Features (PIFs), which is to adjust the image based on the regression result.\n\n\n3. Orthorectification correctionSource:Apollo Mapping, 2016\nCorrection Techniques: Cosine correction Explanation: According to the image, the street looks curve, even if the reality is the straight street. Therefore, cosine correction can be applied to remove distortions and making the pixels viewed at nadir (straight down).\n\n\n\n\n\n\n\nThere are some specific terms used in remote sensing. We summarise some of them which are usually found in the research paper.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nDigital number (DN)\nIntensity of the electromagnetic radiation per pixel\n\n\nRadiance\nHow much light the instrument sees in meaningful units\n\n\nReflectance\nthe ratio of light leaving the target to amount striking the target.\n\n\nTOA reflectance\nMeasure radiation going down and up in a ring\n\n\nSurface reflectance\nMeasure radiation going down\n\n\nhemispherical reflectance\nAll of the light leaving the surface goes straight to the sensor\n\n\n\n\n\n\nIn this process, we will try to extract more features or emphasize certain features, similar to features engineering. We can also apply to imagery to improve the visual appearance. \n\n\n\nEnhancements\nDescription\n\n\n\n\n1. RatioSource: PhysicsOpenLab\nThere are many indices derived from band rationing. They can be used to highlight areas with some spectral trait. One of the most popular indices derived by rationing is the Normalised Difference Vegetation Index (NDVI). It can be used to classify between healthy vegetation and unhealthy vegetation areas. We can calculate NDVI from NIR and RED bands by applying the equation as follows: \\[NDVI= \\frac{NIR-Red}{NIR+Red}\\] Because the NDVI index is based on the concept that healthy and green vegetation reflects more in the NIR but absorbs in the Red wavelength, the new metric is highly useful.There are tons of remote sensing indices. More indices can be found here: https://www.indexdatabase.de/\n\n\n2. FilteringSource: Dwain Casey\nFiltering refers to moving window operations and save as a separate raster file, such as Laplacian filter. There are two types of filtering, which are 1. Low pass - averages the surrounding pixels 2. High pass - enhance local variations\n\n\n3. Edge enhancement\nEdge enhancement is the process to make edges appear in a shaded relief. It is particularly useful for images that have low contrast or are affected by noise, where it can be difficult to distinguish between adjacent features.\n\n\n4. Data fusion\nData fusion is the process of appending new raster data to the existing data (stacking).\n\n\n5. Texture analysisSource: Harris\nTexture analysis examines patterns, structures, and variations in the texture of an image, which can be used to distinguish different features on the ground. Texture refers to spatial variation of gray values."
  },
  {
    "objectID": "week3.html#applications",
    "href": "week3.html#applications",
    "title": "3  Remote sensing data",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nIn this section, we will give examples of remote sensing research and analyse data pre-processing and image enhancement steps in the research.\n\n\nLand cover and land use classification performance of machine learning algorithms in a boreal landscape using Sentinel-2 data (Abdi 2019)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Abdi 2019\n\n\n\n\n\nSummary: It aims to conduct land use classification and compares the classification performance of four machine learning algorithms (support vector machines, random forests, extreme gradient boosting, and deep learning) in a complex boreal landscape in south-central Sweden using Sentinel-2 satellite data. The satellite imagery used for the classification were multi-temporal scenes from Sentinel-2 and the study area consists of eight land-cover and land-use classes. The researcher achieved the overall accuracy of (0.758 ± 0.017) by using support vector machines.\nData Pre-processing:\n\nAtmospheric correction: to converts the top-of-atmosphere reflectance Level-1C data to a bottom-of-atmosphere (BOA) reflectance\nRatio: The normalized difference vegetation index (NDVI), the modified normalized difference water index (MNDWI) and the normalized difference built-up index (NDBI) were calculated to help capture vegetation, water, and artificial surfaces, respectively.\nData Fusion: All calculated bands were appended in each Sentinel-2 scene. Therefore, the total layers of each scene are 13 layers.\n\nComments: The workflow of land cover and land use classification is quite good and complete. There are many data pre-processing techniques used in this work. However, the accuracy of classification in some classes is still not satisfactory, such as to identify open land. The producer’s accuracy of locating open land is only (0.44 ± 0.05). More indices from rationing should be used to help classify this class, such as SAVI, EVI and NDRE (Radočaj et al. 2020).\n\n\n\n\n\n\nSource: Radočaj, D. 2020"
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "3  Remote sensing data",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\n\nContent: There are many techniques of data pre-processing which are reviewed in this week. Although I conducted many machine learning projects, there are some data correction or image enhancement techniques that I forgot to do. After considering every data pre-processing techniques and apply the suitable techniques in each task, it is expected that the accuracy of the machine learning model or the quality of outputs would improve more or less.\nApplication: Since there are so many indices of remote sensing to use from rationing techniques, it is good to see which indices researchers selected to use. Previously, my research project is related to agriculture, so I performed crop classification by using only indices to capture healthy vegetation, such as NDVI. In this week, I have learnt so many new remote sensing indices, which could be useful in the future. However, one question in my mind is that deep learning is known for its ability of automatic feature extraction (Shaheen, Verma, and Asafuddoula 2016). I would like to know whether convolutional neural network or other deep learning models can automatically extract remote sensing indices by itself and even some undiscovered indices that may help improve the accuracy.\n\n\n\n\n\nRadočaj, Dorijan, Jasmina Obhođaš, Mladen Jurišić, and Mateo Gašparović. 2020. “Global Open Data Remote Sensing Satellite Missions for Land Monitoring and Conservation: A Review.” Land 9 (11): 402. https://doi.org/10.3390/land9110402.\n\n\nShaheen, Fatma, Brijesh Verma, and Md. Asafuddoula. 2016. “Impact of Automatic Feature Extraction in Deep Learning Architecture.” In 2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA), 1–8. https://doi.org/10.1109/DICTA.2016.7797053."
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "4  Policy Applications",
    "section": "",
    "text": "This week, we will give an example of urban policies and how to incorporate remotely sensed data to achieve their plans."
  },
  {
    "objectID": "week4.html#applications",
    "href": "week4.html#applications",
    "title": "4  Policy",
    "section": "4.2 Applications",
    "text": "4.2 Applications"
  },
  {
    "objectID": "week4.html#reflection",
    "href": "week4.html#reflection",
    "title": "4  Policy Applications",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nThis week’s lessons have been helpful in understanding the larger process of urban plans and improvement, of which remote sensing is an essential part. There are also global issues that need to be addressed. However, one solution may not fit all, due to the difference between areas. This lesson also shows how each local government address the similar issues in different ways. Ultimately, I could see the concrete action plans to address each urban issue from New York City’s strategic plan, which could be adopted to partially used for addressing similar issues in my country too. Therefore, I may have to find time analysing each policy and implementation steps again to find which implementation plan I can follow the steps and apply in my country or which one should I adjust some steps before using to make it suitable to my city.\n\n\n\n\nRolnick, David, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, et al. 2022. “Tackling Climate Change with Machine Learning.” ACM Computing Surveys 55 (2): 42:1–96. https://doi.org/10.1145/3485128.\n\n\nUS EPA, OAR. 2015. “Sources of Greenhouse Gas Emissions.” Overviews and {Factsheets}. https://www.epa.gov/ghgemissions/sources-greenhouse-gas-emissions."
  },
  {
    "objectID": "week4.html#summary",
    "href": "week4.html#summary",
    "title": "4  Policy",
    "section": "4.1 Summary",
    "text": "4.1 Summary\n\nPolicy: OneNYC 2050, New York\n\n\nSelected policies\n\n\nVolume 7 of 9 - A Livable Climate Initiative 20. Achieve Carbon Neutrality And 100 Percent Clean Electricity By 2050, New York City will have net-zero GHG emissions citywide. To achieve this, we will reduce our emissions as much as possible and offset our “irreducible emissions,” GHG emissions by Sector - Building, Transportation, Waste\nVolume 8 of 9 - Efficient Mobility Initiative 26. Reduce congestion and emissions GHG emissions reduction - be able to reduce greenhouse gas (GHG) emissions, support sustainable growth, and achieve the ambitious goal we set in 2015 to have 80 percent of all trips in the city taken by sustainable modes by 2050 Green The fleet - Near term emissions reductions will be achieved by implementing renewable diesel fuel, accelerating the transition to EV and hybrid vehicles, and increasing the efficiency of the fleet, which will help reduce the City’s fuel consumption to below 2014 levels."
  },
  {
    "objectID": "week4.html#policies-summary",
    "href": "week4.html#policies-summary",
    "title": "4  Policy Applications",
    "section": "4.1 Policies Summary",
    "text": "4.1 Policies Summary\n\nPolicy: OneNYC 2050, New York\n\n\nSelected policies\n\n\nVolume 7 of 9 - A Livable Climate Initiative 20: “Achieve Carbon Neutrality And 100 Percent Clean Electricity”\n\nGoal: “By 2050, New York City will have net-zero greenhouse gas (GHG) emissions citywide. To achieve this, we will reduce our emissions as much as possible and offset our irreducible emissions”\nAs transportation is the main cause of greenhouse gas emission in the United States (US EPA 2015), we will further look on the policies related to transportation for further investigating how can we amend the situation of greenhouse gas caused by transports.\n\n\n\n\n\n\n\n\nSource: US EPA, OAR. 2015\n\n\nVolume 8 of 9 - Efficient Mobility Initiative 26: “Reduce congestion and emissions”\n\nGoal: “GHG emissions reduction” - be able to reduce greenhouse gas (GHG) emissions, support sustainable growth, and achieve the ambitious goal we set in 2015 to have 80 percent of all trips in the city taken by sustainable modes by 2050\n“Green The fleet” - Near term emissions reductions will be achieved by implementing renewable diesel fuel, accelerating the transition to EV and hybrid vehicles, and increasing the efficiency of the fleet, which will help reduce the City’s fuel consumption to below 2014 levels."
  },
  {
    "objectID": "week4.html#applications-using-remote-sensing",
    "href": "week4.html#applications-using-remote-sensing",
    "title": "4  Policy Applications",
    "section": "4.2 Applications using Remote Sensing",
    "text": "4.2 Applications using Remote Sensing\n\n\n\n\n\n\nSource: Rolnick 2022\n\nThere are many opportunities to reduce GHG emissions from transportation using machine learning and remote sensing. (Rolnick et al. 2022)\nIn our case study, we aim to incorporate remote sensing to reduce transportation activity and track the use of EV and hybrid vehicle which will result in CO2 emission reduction. Our proposed methodology is to monitor CO2 and transport mode in each area, then we can summarise the statistics of transports used in each area along with CO2 emission in each area too. According to our goal to make the transition to EV and hybrid vehicles, this information will benefit policymaker to track the progress of transition and CO2 emission reduction in each area.\nGoal: Predict change in CO2 from switching behaviour in transport mode for decarbonizing transportation\nSubtask:\n\nestimate average vehicle traffic\nclassify vehicle types\nmonitor CO2 emission\n\nData:\n\nRemote Sensing data\n\nWorldwide but lower resolution e.g. Sentinel-1/2, Landsat\nHigh resolution e.g. Vehicle Detection in Aerial Imagery (VEDAI)\n\nvehicle label for detection and classification\nCO2 Detected from Satellite data, e.g. Orbiting Carbon Observatory-3 (OCO-3)\n\nProposed Solutions:\n\nTask1: estimate average vehicle traffic\n\nIssue: Traditionally, traffic is monitored with ground based counters that are installed on selected roads. As ground-based counters require costly installation and maintenance, many countries do not have such systems.\nMethodology: Vehicles can also be detected in high-resolution satellite images with high accuracy, and image counts can serve to estimate average vehicle traffic\nData: Remote Sensing data and vehicle label for detection\n\nTask2: classify vehicle types\n\nMethodology: Use convolutional neural network (CNN) with satellite imagery to classify vehicle types\nData: Remote Sensing data and vehicle label for classification\nSample project: https://github.com/AlperenCicek/vehicle-detection-from-satellite\n\nTask3: monitor CO2 emission\n\nMethodology: Having real-time maps of GHGs (detected from remote sensing) could help us quantify CO2 emissions. For example, data on emissions make it possible to set effective targets, and pinpointing the sources of emissions makes it possible to enforce regulation\nData: Orbiting Carbon Observatory-3 (OCO-3) - measures and maps column CO2 in great detail\n\n\nGlobal agendas:\nAccording to United Nations 2030 Sustainable Development Goals (SDGs) agenda, our approach could help enable 2 SDGs which are\n\nSDG 11: Sustainable Cities and Communities\n\nreduce greenhouse gas emissions - a quarter of energy-related global greenhouse gas emissions come from transport and that these emissions are projected to grow substantially in the years to come\n\nSDG 13: Take urgent action to combat climate change and its impacts\n\nreaching net-zero carbon dioxide CO2 emissions globally by 2050"
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "5  Introduction to Google Earth Engine",
    "section": "",
    "text": "This weeks, we will talk about one of the most powerful platforms to implement remote sensing applications, which is Google Earth Engine.\n\n\n\n\n\n\nSource: Google Earth Engine\n\n\n\nGoogle Earth Engine (GEE) is a platform for the cloud-based processing of scientific data. The key feature that attracts the majority of GIS users is its ability to perform large computations extremely quickly, as most remote sensing data are quite huge and it is difficult to conduct research on a very broad region. Moreover, it also provides the visualisation panel that we can run the code and see the output from the map in a minute. The coding language used in the platform is javascript. However, they also provide Google Earth Engine Python API for users who get familiar with python.\nThe interface of Google Earth Engine platform is shown below:\n\n\n\n\n\n\nSource: GEE community Beginner’s Cookbook, TC25\n\n\n\n\nEarth engine has its own data types or terms, which are explained as follow:\n\n\n\n\n\n\nSource: GEE\n\n\nData types in Google Earth Engine\n\nImage = Raster\nFeature = Vector\nImageCollection = Image stack\nFeatureCollection = Feature stack\n\n\n\n\n\nThe programming language used in GEE is javascript. We can use any basic functions just like what we used to do. Some sets of functions are run on the server side, such as ee.ImageCollection([PATH]), so we can tackle with very large datasets. However, there is a few warnings, which should be aware of. One thing that should be remembered is looping. Don’t use looping function on the server, since it does not allow earth engine to run distributed for a quick process. The alternative function of looping is map().\nExample of using map() to capitalise instead of for() for looping:\n\n# map\nnames.map(name => capitalize(name))\n\nMoreover, there are many geometry operations to help process data, including joins, zonal statistics (e.g. average temperature per neighbourhood), filtering of images or specific values.\n\nWe can both upload our own datasets, which can be shapefile, into GEE assets and use them or select data from Earth Engine data catalog.\nThe example of loading data, filtering specific periods and bounding areas is provided below:\n\nvar india = ee.FeatureCollection('projects/ucl-remote-sensing/assets/india_boundary')\n    .filter('GID_1 == \"IND.25_1\"');\n\nvar oneimage_study_area_cloud = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n  .filterDate('2021-06-01', '2022-10-10')\n  .filterBounds(india)  // Intersecting ROI\n  .filter(ee.Filter.lt(\"CLOUD_COVER\", 0.1));\n\n\nThere are many functions for processing raster and vector data provided in GEE. We will show examples of some functions that are usually used in the preprocessing steps.\n\nMosaic images: taken images according to their order in the collection (last on top)\n\n\nvar mosaic = oneimage_study_area_cloud_scale.mosaic();\n\nvar vis_params2 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0.0,\n  max: 0.3,\n};\n\nMap.addLayer(mosaic, vis_params2, 'spatial mosaic');\n\n\nClip images: filter images from a given areas specified in feature/shapefile/vector.\n\n\nvar clip = meanImage.clip(india)\n  .select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7']);\n\nvar vis_params3 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0,\n  max: 0.3,\n};\n\n// map the layer\nMap.addLayer(clip, vis_params3, 'clip');\n\n\nCalculate statistics: such as median\n\n\nvar median = oneimage_study_area_cloud.reduce(ee.Reducer.median());"
  },
  {
    "objectID": "week5.html#applications",
    "href": "week5.html#applications",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nAfter we have finished conducting our own analysis, we can also publish our analysis via Google Earth Engine Apps, which provide ability to turn code into responsive applications. Some great examples are selected to present below:\n\n\nEurope’s Air Quality Winner (Source: gaertnerp)\n\n\n\n\n\n\n\n\nSummary: This application was built to visualise the improvement of air quality in Europe after the lockdown due to the corona pandemic. The amount of NO2 was captured by remote sensing and shown in chrolopleth map. Users can compare the situation between 2 periods and The difference of NO2 median composite was calculated and shown.\nPros: it is quite great visualisation that users can understand how much the air quality changes in a minute.\nCons: it is not allowed to zoom-in to see more details in each specific area.\n\n\n\nNDVI Slider (Source: khaledalshamaa)\n\n\n\n\n\n\n\n\nSummary: The developer aims to compare Normalized Difference Vegetation Index (NDVI) between 2 years. The source of data is MODIS, which has the spatial resolution of 250 m and temporal resolution of 16-day.\nPros: In this work, it is better than the previous work above in aspect that users can zoom-in to see NDVI in specific area.\nCons: Currently, users can select to view only the average of NDVI of each year. However, NDVI values are quite vary within the same year. If the system has the ability to view data by seasons (e.g. Autumn, Winter) or date picker to let users choose the period they want to see, it will be better."
  },
  {
    "objectID": "week5.html#reflection",
    "href": "week5.html#reflection",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\n\nTools: I found that Google Earth Engine is a very powerful platform since it provides server side functions and large servers for high computation which is best fit with remote sensing projects that usually require big data for conducting researches. Moreover, it also provides UI for visualising maps from our codes. Before knowing the available of GEE, I found that it is quite hard to conduct remote sensing products since it took a very long time to conduct a research from data collection to data processing. Moreover, Earth Engine Data Catalog is very useful for the use of data. It provides many common remote sensing data that can be used for analysis in many tasks. While I am using many data from Google Earth Engine data datalog, I found that there is the other service offered from AWS called “Earth on AWS”, which also provide remote sensing data in the cloud service too. If there is some data unavailable in EE data catalog, I may check if it is available in AWS service too. Then, further investigation may include make a comparison between 2 platforms.\nApplications: Earth Engine App is very convenient for publishing analysis to let other people to see our results. It is also good for exploring data and finding insight. The great part of it is the interactive part which allow users to adjust parameters and see the results."
  },
  {
    "objectID": "week8.html#applications",
    "href": "week8.html#applications",
    "title": "8  Urban Heat Island",
    "section": "8.2 Applications",
    "text": "8.2 Applications\nNext, we will see how can we tackle with this critical issue.\n\n8.2.1 Policy and Implementation\n\nGlobal Policy\n\n\nThe Sustainable Development Goals (SDGs) provided by UN also recognised the critical situation of this issue and call to action to ameliorate this issue.\nGoal 11: “Make cities and human settlements inclusive, safe, resilient and sustainable”\nSDG 11 issue brief: “Investing in parks and green spaces in urban areas will help to ameliorate the urban heat island effect and improve air quality in urban spaces.”\nIt can be seen that UN also recognised this global challenge and underlined this issue in the issue brief document. However, there is still no targets to address this issue directly.\n\n\nLocal Policy\n\nThere are many cities choose to issue the policy and strategy to amend the situation. Here are some examples:\n\n\n\n\n\n\n\n\n\nCity\nStrategy\nAction\nOutcomes\n\n\n\n\nBarcelona, Spain\nSuperblocks model\nReorganizes the city’s streets to prioritize pedestrians and non-motorized transportation, creating large car-free zones\nReducing traffic, better air quality, free up space previously occupied by cars for allowing for the creation of new parks and also reduce noise in the city\n\n\nMelbourne, Australia\nUrban Forest Strategy, Green Our City Strategic Action Plan\nPlant more trees, improving urban landscapes, and creating green spaces\nIncreased green spaces, tree planting, and green roofs, which have led to improved microclimate, better air quality, and enhanced biodiversity in the city.\n\n\nMedellín, Colombia\nGreen Corridors Project\nCreate green corridors throughout the city by planting trees and vegetation along streets, sidewalks, and waterways\nMore green corridors along streets, riverbanks, and other urban spaces, better air quality, improved microclimate conditions and even reduce temperature 2 degrees of surface temperature.\n\n\nPerth, Australia\nUrban Forest Plan, Greening Plan 2020-2050\nPlant more trees in public spaces and encouraging private property owners to plant trees on their land\nIncreasing tree canopy cover and green spaces, better air quality, and enhanced urban biodiversity\n\n\nTokyo, Japan\nHeat Island Mitigation Act and Tokyo Metropolitan Environmental Security Ordinance\nPromote the use of cool pavements and roofs\nImplemented cool pavements, green roofs, and increased green spaces, which have contributed to reduced urban heat island intensity and improved living conditions for the city’s residents.\n\n\n\nIt could be seen that there are a lot of benefits resulting from environmental policies related to temperature. Therefore, we should consider more about the effects in environments of everything we plan to do in the future."
  },
  {
    "objectID": "week8.html#reflection",
    "href": "week8.html#reflection",
    "title": "8  Urban Heat Island",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\n\nContent: the Urban heat Island effect is quite new to me. Honestly, I have never heard about this issue before, although I am from Thailand, the country with very high temperature. Next step for me is to find out more about this issue in Thailand and what are the implementation plans to tackle with this issue in my country. I researched on this issue a liitle bit and still have not found any documents or policies related to this issue from the government. This lecture enlightened me to realise the other critical issue that should be considered.\nApplications: It is very amazing to see how each city tackle with the Urban Heat Island issue with the different way. The outcomes from each strategy are very promising. I was inspired by the video of Superblocks model shown in the lecture that we can really design our city and there is the room to solve every challenging issues even though how big it is. Moreover, I have just realised that cooler only 2 degrees of temperature can really better our society."
  },
  {
    "objectID": "week7.html#applications",
    "href": "week7.html#applications",
    "title": "7  Classification II",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nIn this section, we will give examples of object based image analysis projects in remote sensing with an analysis.\n\n\nMonitoring land use changes associated with urbanization: An object based image analysis approach (Samal and Gedam 2015)\n\n\n\n\n\n\n\nSource: Samal and Gedam 2015\n\n\n\n\n\n\n\n\n\nSource: Samal and Gedam 2015\n\n\n\n\n\nSummary: This study aims to monitor land use/land cover (LULC) changes due to urbanization in a rapidly changing river basin in India. The researchers analyzed past changes and predicted possible consequences within the river basin’s natural boundaries. Multi-temporal images from Landsat and Indian Remote Sensing (IRS) satellites from 1992 to 2009, as well as a digital elevation model, were used to generate historical and current LULC patterns in the basin. The object-based image analysis technique was employed for precise classification of multi-temporal images, followed by GIS-based change detection studies. The results show that the built-up area increased significantly, adding 288 km² between 1992 and 2009. This increase in built-up area is attributed to a decrease in wastelands and agricultural land. This research also achieved an overall accuracy of 92.7% and kappa index of agreement of 0.91, while they got the producer accuracy of 90% except forest class.\nMethodology: The researcher used an object-based image classification to classify land use by using K-nearest-neighbour into 5 classes; which are agricultural land, built up area, forest cover, wastelands and water bodies, and identified changes from 1992 to 2009. Image segmentation was performed to divides large heterogeneous image into a finite number of homogenous groups before classifying based on their different spectral and textural characteristics, including red, NIR, green and NDVI bands. Metrics used in this research were an overall accuracy, kappa index and producer’s accuracy\nComments: OBIA is an appropriate method to use in this project since it could help to capture the spatial extent of classes like forests or water bodies and then result in improvement of classification performance. The class that got the lowest accuracy was forest, since the models were confused with wetland and agriculture area. Further improvement of this research may include applying more complex models, such as object-based convolutional neural networks (OCNN) (Zhang et al. 2018). There are many researches that found that deep learning models outperform traditional machine learning models like K-NN in classification tasks."
  },
  {
    "objectID": "week7.html#reflection",
    "href": "week7.html#reflection",
    "title": "7  Classification II",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\n\nContent: I am very interesting to study the content of this week, since some of my previous work was to conduct crop classification task using random forest. However, I found the issue that the model did not classify the same class for every pixels in the field. I think that object based image analysis would solve this issue. However, I tried applying SNIC algorithm and found that it was not correctly segmented every fields. It is possible that data used in my research was a low-resolution imagery and it was hard to conduct perfect segmentation. Nevertheless, the outcome was better than pixel-based classification.\nApplications: Land use changes is the other interesting topics that I would like to research more, since I had never conducted this research. The accuracy of this research is also very high (90%), so it means that it is possible that our research about this topic will be succeeded. Moreover, I would like to know more about what kind of complex urban land use which complex model such as object-based CNN (OCNN) can perform much better than other common used models e.g. random forest. It could be one of my list of future researches.\n\n\n\n\n\nAchanta, Radhakrishna, Appu Shaji, Kevin Smith, Aurélien Lucchi, Pascal Fua, and Sabine Süsstrunk, eds. 2010. SLIC Superpixels. EPFL.\n\n\nSamal, Dipak R., and Shirish S. Gedam. 2015. “Monitoring Land Use Changes Associated with Urbanization: An Object Based Image Analysis Approach.” European Journal of Remote Sensing 48 (1): 85–99. https://doi.org/10.5721/EuJRS20154806.\n\n\nZhang, Ce, Isabel Sargent, Xin Pan, Huapeng Li, Andy Gardiner, Jonathon Hare, and Peter M. Atkinson. 2018. “An Object-Based Convolutional Neural Network (OCNN) for Urban Land Use Classification.” Remote Sensing of Environment 216 (October): 57–70. https://doi.org/10.1016/j.rse.2018.06.034."
  },
  {
    "objectID": "week6.html#applications",
    "href": "week6.html#applications",
    "title": "6  Classification I",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nIn this section, we will give examples of classification projects in remote sensing with an analysis.\n\n\nEstimation of seismic building structural types using multi-sensor remote sensing and machine learning techniques (Geiß et al. 2015)\n\n\n\n\n\n\n\nSource: Geiß et al. 2015\n\n\n\n\n\nSummary: This research aims to classify seismic building structural types (SBSTs) in earthquake-prone regions from multi-sensor remote sensing data. This research was conducted by using large parts of Padang city in Indonesia as the study areas. Data used in this project include a multispectral IKONOS image with 4 m of spatial resolution for the 4 multispectral bands, multitemporal Landsat data with 30 m spatial resolution for the 7 multispectral bands and height information from a digital surface model (DSM). The SBSTs classification models with 30 extracted features resulted in an overall accuracy of 72% and 75% for using random forest and SVM, respectively.\nMethodology: A set of features is derived from remote sensing data at two different spatial levels, building and block level. Then, researchers chose to conduct outlier identification by using one-class support vector machines (OC-SVM) to exclude unreliable data. After that, they classified building by using hierarchical supervised classification. The machine learning models used in this classification were support vector machine (SVM) and random forests.\nComments: It is a good and useful research that demonstrated the potential of using machine learning to classify building structural types based on remote sensing data. Many features were extracted for using in classification models. The comparison of the performance between random forest and SVM was also shown in the results. Possible improvements may include applying image enhancement techniques and other data pre-processing techniques before training models. Texture analysis probably help in this research."
  },
  {
    "objectID": "week6.html#reflection",
    "href": "week6.html#reflection",
    "title": "6  Classification I",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\n\nContent:\n\nThere are many machine learning models taught in this week. As a data scientist, I have already used many models in many projects before. However, I sometimes forgot some implementation steps, since we have libraries, such as sklearn in python. The use of model is just typing one line of code and the magic happens. So, it is good to review all of the concepts of each model again. Moreover, knowing pros and cons of each model will help improve the quality of machine learning project design. Sometimes, I focused on only accuracy too much and forget to think about interpretability of models.\nIn term of concepts, I found that ensembling and bagging techniques in random forest model are quite interesting, since it could be seen that the winner of many machine learning competitions also use these techniques. I wonder if we can use the concept of ensembling with many SVM models and how much the performance will increase resulting from adding this technique. I expect that it will slightly increase the performance, however, it will take much longer time for computation. There was also one research to support my idea (Claesen 2014).\n\nApplications:\n\nCurrently, I am interested in researching about damage building assessment from satellite imagery. Therefore, this research looks interesting to me, since it also conducted building classification. The workflow in this project also involved many techniques before achieving the promising accuracy.\nFor specific technique, I used SVM as a classification model for many projects, but I have not used one-class svm (OC-SVM) for identifying outliers, since there are also many techniques for excluding outliers, such as using mean and standard deviation method or isolation forest (Liu, Ting, and Zhou 2008). So, it is good to explore the performance of OC-SVM compared to other methods.\n\n\n\n\n\n\nClaesen, Marc. 2014. “EnsembleSVM: A Library for Ensemble Learning Using Support Vector Machines.”\n\n\nGeiß, Christian, Patrick Aravena Pelizari, Mattia Marconcini, Wayan Sengara, Mark Edwards, Tobia Lakes, and Hannes Taubenböck. 2015. “Estimation of Seismic Building Structural Types Using Multi-Sensor Remote Sensing and Machine Learning Techniques.” ISPRS Journal of Photogrammetry and Remote Sensing 104 (June): 175–88. https://doi.org/10.1016/j.isprsjprs.2014.07.016.\n\n\nLiu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. 2008. “Isolation Forest.” In 2008 Eighth IEEE International Conference on Data Mining, 413–22. https://doi.org/10.1109/ICDM.2008.17."
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "6  Classification I",
    "section": "",
    "text": "In this week, we will look into classification projects in remote sensing. Classification is one of the primary tasks in remote sensing. It can be done based on their spectral, spatial, or temporal characteristics.\n\n\nThere are many applications in remote sensing utilising classification techniques. Here are some example projects:\n\nLand Cover Classification: It aims to categorise the earth’s surface into different classes, such as urban areas, forests, water bodies, agriculture, and wetlands, to monitor changes in land use, assess environmental impacts, or develop land use policies.\nUrban Change Detection: A task that identifies changes in urban areas, such as urban expansion, urban green space loss, and land use alterations, to inform urban planning, assess the impact of urbanization, and monitor city growth.\nAgricultural Monitoring: A project that classifies agricultural lands by crop type or irrigation status, assesses crop health, and monitors agricultural practices, to support food security, optimize resource use, and inform agricultural policies.\nForest Monitoring and Illegal Logging: A project that classifies forest types, monitors forest health, and detects deforestation, to support sustainable forest management and climate change mitigation efforts.\n\nNext, we will walk through machine learning models for classification and clustering tasks.\n\n\n\n\n\n\n\nSource: geeksforgeeks\n\n\nOverview: CART is a decision tree-based learning algorithm with multiple decision rules, which are derived from the data features. It can be used for both classification and regression tasks.\nThings to consider:\n\nGini impurity: We use the Gini impurity at each branch to split the nodes further and models are selected from the lowest impurity. \\[Gini\\ impurity= 1-(probability\\ of\\ yes)^2-(probability\\ of\\ no)^2\\]\nOverfitting: It can be easily to face with overfitting issues. Some techniques to solve this issue is to limiting how trees grow (e.g. a minimum number of pixels in a leaf, 20 is often used) and weakest link pruning.\n\nPros: able to handle both continuous and categorical variables, interpretability through its tree structure, and robustness to outliers.\nCons: overfitting may occur, especially with high-dimensional data, and may require parameter tuning and pruning to optimize its performance\n\n\n\n\n\nSource: Science Direct\n\n\nOverview: An ensemble learning method that combines multiple decision trees to improve classification or regression performance. The method is based on the concept of bagging (Bootstrap Aggregating), where each decision tree is trained on a random subset of the training data with replacement. The answer comes from voting system (majority decision).\nThings to consider:\n\nNumber of trees: More trees generally improve the model’s performance but may increase computational costs.\nTree depth: Controlling tree depth can help balance model complexity and generalization.\nNumber of features to consider at each split: This parameter affects the diversity of individual trees and can impact the model’s performance.\n\nPros: It is known for its robustness to overfitting, ability to handle high-dimensional data, and good performance in both classification and regression tasks in compared with single decision trees.\nCons: It is difficult to interpret and have longer prediction times due to the combination of multiple trees.\n\n\n\n\n\nSource: Núñez et al. 2018 High-Resolution Satellite Imagery Classification for Urban Form Detection\n\n\nOverview: a supervised learning algorithm with the goal to find an optimal hyperplane that separates different classes in the feature space, maximizing the margin between classes. SVM can also handle both linear and non-linear relationships between features using kernel functions. There is also an ability to allow some misclassification to occur (soft margin).\nThings to consider:\n\nRegularization parameter (C): A larger value of C leads to a smaller margin but fewer errors, while a smaller value allows for a larger margin but more errors.\nKernel coefficient (Gamma): It controls the shape of the decision boundary and the degree of non-linearity in the model. Low gamma will result in big radius for classified points, vice versa.\nKernel function: It determines how the input features are transformed. There are many kernel functions, such as linear, polynomial and RBF. It can significantly impact the performance of the SVM model. Note that linear and RBF kernel are popular choices for linear and non-linear problems, respectively.\n\nPros: SVM can handle linear and non-linear boundaries using different kernel functions.Moreover, SVM is less prone to overfitting, especially when using an appropriate regularization parameter.\nCons: As its complexity, it is challenging to interpret the model, especially with non-linear kernels. Furthermore, SVM can be computationally expensive, particularly with large datasets and non-linear kernels."
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "7  Classification II",
    "section": "",
    "text": "In this week, we are going to look at segmentation and classification tasks in remote sensing which are object based image analysis and sub pixel analysis. Moreover, all the accuracy assessments will be reviewed.\n\n\n\n\n\n\n\n\nSource: GISGeography\n\n\nOverview: Object-Based Image Analysis (OBIA) is an approach to classify groups of pixels or image objects that share similar characteristics, such as spectral, spatial, or textural properties. The main tasks in OBIA consist of segmentation and classification.\nThings to consider: In order to grouping pixels into superpixels, we consider based on the similarity (homogeneity) or difference (heterogeneity) of the cells.\nPros: It better representation of complex spatial patterns, and the ability to incorporate contextual information. Moreover, an classification accuracy may be increased in compared with performing pixel-based classification.\nCons: It can also be computationally intensive and may require more complex algorithms and expert knowledge to achieve optimal results\nSegmentation Algorithms:\n\nSimple Linear Iterative Clustering (SLIC) Algorithm (Achanta et al. 2010)\n\nIt is mainly used for superpixel segmentation by clustering pixels based on two considerations which are\n\nhomogenity of colours: color similarity\ncloseness to centre: spatial distance from point to centre of pixel\n\nThe algorithm is summarised as follow: \n\n\n\n\nSource: Achanta et al. 2010\n\n\n\n\n\n\n\n\n\n\nSource: MacLachlan et al. 2017\n\n\nOverview: Sub pixel analysis or spectral mixture analysis (SMA) is used to analyze and interpret the spectral signatures of mixed pixels in multispectral for dealing with images that have lower resolution than the features of interest on the ground. Since a single pixel may contain more than one type of land cover, object, or feature; the amount of sub-pixels for each class is calculated, before they are spatially allocated. SMA aims to estimate the abundance or proportion of each component (endmembers).\nThings to consider:\n\nPixel purity : Consider if a pixel contains a single land cover type or material or mixed signatures\nNumber of Endmembers: the number of endmembers should represent the actual number of distinct land cover types or materials in the study area. Too few endmembers may lead to under-representation of the true composition of mixed pixels, while too many endmembers can result in overfitting and increased complexity. In urban areas, the Vegetation-Impervious surface-Soil (V-I-S) model is usually used.\n\nPros: Sub pixel analysis can improve the classification accuracy, especially for low-resolution or noisy images. It also may enhance the detection of subtle changes in land cover or other features over time, which may not be noticeable at the native image resolution.\nCons: Sub-pixel analysis relies on the assumption that the spectral signature of a mixed pixel is a linear combination of the spectral signatures of its constituent components. This assumption may not always hold, as interactions between materials and land cover types can cause non-linear spectral mixing, which can complicate the analysis and lead to inaccuracies. Moreover, the accuracy of sub-pixel analysis is heavily dependent on the quality and representativeness of the endmembers.\nSpectral mixture analysis steps:\n\nSelect endmembers\nestimate abundance of each endmember within the mixed pixels by solving a linear system of equations\n\nThe outputs will be the fraction of each class within one pixel.\n\n\n\n\nThere are many metrics to evaluate the performance of the classification models.\n\n\n\n\n\n\nSource: Barsi et al. 2018\n\nSome of the most widely used metrics include:\n\n\n\n\n\n\n\n\nMetrics\nFormula\nDefinition\n\n\n\n\nProducer’s accuracy (PA)\n\\[PA= \\frac{TP}{TP+FN}\\]\nIt is the same as recall or true positive rate or sensitivity. It refers to the number of correctly classified pixels for a given class divided by the total number of reference pixels for that class\n\n\nUser’s Accuracy (UA)\n\\[PA= \\frac{TP}{TP+FP}\\]\nIt is the same as precision or positive predictive value. It refers to the number of correctly classified pixels for a given class divided by the total number of classified pixels for that class\n\n\nThe overall accuracy (OA)\n\\[PA= \\frac{TP+TN}{TP+FP+FN+TN}\\]\nThe number of correctly classified pixels divide by the total number of pixels\n\n\nKappa Coefficient\n\\[\\kappa = \\frac{P_o - P_e}{1 - P_e}\\]\nIt expresses the accuracy of an image compared to the results by chance. It ranges from 0 to 1. Note that po is the proportion of cases correctly classified (accuracy) and Pe expected cases correctly classified by chance.\n\n\nF1-Score\n\\[F1 = \\frac{2*Precision*Recall}{Precision+Recall}\\]\nThe F1-Score (or F Measure) combines both recall (Producer accuracy) and Precision (User accuracy)"
  },
  {
    "objectID": "week8.html",
    "href": "week8.html",
    "title": "8  Urban Heat Island",
    "section": "",
    "text": "In this week, we will present some environmental issue related to temperature, which is the Urban Heat Island, and may need an involvement of remote sensing for the solutions."
  },
  {
    "objectID": "week8.html#summary",
    "href": "week8.html#summary",
    "title": "8  Urban Heat Island",
    "section": "8.1 Summary",
    "text": "8.1 Summary\n\n8.1.1 What is it?\n\n\n\n\n\n\nSource: World Meteorological Organization (WMO)\n\nThe Urban Heat Island (UHI) effect refers to the phenomenon where urban areas experience higher temperatures than their surrounding rural areas. This occurs mainly due to the replacement of natural vegetation with built surfaces, such as buildings, roads, and other infrastructure, which absorb and retain more heat. The UHI effect can exacerbate energy consumption, air pollution, and greenhouse gas emissions, as well as negatively impact human health and comfort.\n\n\nThe causes of Urban Heat Island\n\nThere are two main causes of UHI which are following:\n\nHeat-absorbing materials: Buildings, roads, and other infrastructure in urban areas absorb and re-radiate heat, contributing to increased temperatures. In other words, there are more dark surfaces that retain heat\nLess vegetation: Urban areas have less vegetation, which reduces the amount of shade and cooling through evapotranspiration, leading to increased temperatures.\n\nthe other causes of this issue are following:\n\nUrban geometry: The arrangement and density of buildings in urban areas can affect air circulation and solar radiation absorption. Tightly packed buildings, narrow streets, and tall structures can trap heat and limit the movement of cool air, exacerbating the UHI effect.\nHeat Generated from Human Activities: Various human activities in urban areas, such as transportation, industrial processes, and air conditioning systems, generate waste heat, which contributes to increased temperatures in cities.\nReduced wind flow: Urban structures can obstruct and slow down wind flow, which would otherwise help disperse heat and bring in cooler air from surrounding areas. This reduced wind flow contributes to the buildup of heat in urban environments.\n\n\nTo know more about Urban Heat Island, check this video made by CASA students: Urban Heat Island Interpretive Dance\n\n\n8.1.2 Urban Heat Island Situations Around the World\n\nIn developed countries: Extensive research has been conducted on the UHI effect in major cities such as New York, London, Tokyo, and Sydney. These cities often have large impervious surfaces, high building density, and limited green spaces, contributing to the UHI effect. Governments and city planners in these regions have been implementing various mitigation strategies, including increasing urban greenery, promoting energy-efficient buildings, and optimizing urban design.\nIn developing countries: the UHI effect is often more pronounced due to rapid urbanization, uncontrolled land use changes, and limited resources for sustainable urban planning. Cities such as Delhi, Beijing, São Paulo, and Lagos experience severe UHI effects, compounded by the lack of green spaces, poor air quality, and inadequate infrastructure. In these cities, addressing the UHI effect is crucial for improving public health, energy efficiency, and overall quality of life."
  }
]