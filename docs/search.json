[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Remotely Sensing Cities and Environments",
    "section": "",
    "text": "This is a learning diary of the module CASA0023: Remotely Sensing Cities and Environments offered by UCL. More details: https://andrewmaclachlan.github.io/CASA0023/\n\n\n\n\nJames - Nutthaphol Rakratchatakul\nStudent ID: 21174510\nMsc Urban Spatial Science\n\n\n\n\n\n\n\n\nJames’ research interest includes agricultural intelligence for sustainable planning, including combining satellite imagery and machine learning to help them monitor crop health, suggest how to optimize their resources and warn them before the disaster is coming.\n\n\n\n\nMSc., Urban Spatial Science, University College London (Present)  B.E., Computer Engineering, Chulalongkorn University (2018)"
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  SAR Sensor",
    "section": "",
    "text": "Slide: URL"
  },
  {
    "objectID": "week2.html#characteristics",
    "href": "week2.html#characteristics",
    "title": "2  SAR Sensor",
    "section": "2.2 Characteristics",
    "text": "2.2 Characteristics\n\n30m spatial resolution\nrevisit period is 16 days\n\nhttps://andrewmaclachlan.github.io/CASA0023-lecture-1/#1"
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "1  An Introduction to Remote Sensing",
    "section": "",
    "text": "There are two types of sensor which are active and passive sensors. Active sensors, such as Human eye, camera, satellite sensor, Use energy that is available and usually detect reflected energy from the sun. On the other hand, active sensors, such as Radar, X-ray and LiDAR, have an energy source for illumination and actively emit electromagnetic waves and then wait to receive.\n\n\nMost common data format is raster. Common file types include BIL, BSQ, BIP and GeoTIFF.\nResolutions of data are considered in many aspects, including spatial, spectral, temporal and radiometric. The example of resolutions is provided below:\n\n\n\n\n\n\n\n\nResolution\nExample\n\n\n\n\nSpatial\n30 m\n\n\nSpectral\nBand 4 - Red, Band 5 - Near Infrared (NIR), Band 6 - SWIR 1\n\n\nTemporal\ndaily\n\n\nRadiometric\nan 8 bit sensor has values between 0 and 255 (256 possible values)"
  },
  {
    "objectID": "SAR_presentation.html",
    "href": "SAR_presentation.html",
    "title": "Synthetic Aperture Radar (SAR)",
    "section": "",
    "text": "SAR Applications\n\n\n\n\n\n\n\n\n\nhttps://www.l3harrisgeospatial.com/Learn/Blogs/Blog-Details/ArtMID/10198/ArticleID/24031/Enhancing-Situational-Awareness-with-SAR-Data\n\n\n\nReflection"
  },
  {
    "objectID": "sample_xaringan_presentation.html",
    "href": "sample_xaringan_presentation.html",
    "title": "Presentation Ninja",
    "section": "",
    "text": "???\nImage credit: Wikimedia Commons\nclass: inverse, center, middle\n\nGet Started\n\n\n\nHello World\nInstall the xaringan package from Github:\n\nremotes::install_github(\"yihui/xaringan\")\n\n–\nYou are recommended to use the RStudio IDE, but you do not have to.\n\nCreate a new R Markdown document from the menu File -> New File -> R Markdown -> From Template -> Ninja Presentation;1\n\n–\n\nClick the Knit button to compile it;\n\n–\n\nor use the RStudio Addin2 “Infinite Moon Reader” to live preview the slides (every time you update and save the Rmd document, the slides will be automatically reloaded in RStudio Viewer.\n\n.footnote[ [1] 中文用户请看这份教程\n[2] See #2 if you do not see the template or addin in RStudio. ]\n\n\nHello Ninja\nAs a presentation ninja, you certainly should not be satisfied by the “Hello World” example. You need to understand more about two things:\n\nThe remark.js library;\nThe xaringan package;\n\nBasically xaringan injected the chakra of R Markdown (minus Pandoc) into remark.js. The slides are rendered by remark.js in the web browser, and the Markdown source needed by remark.js is generated from R Markdown (knitr).\n\n\n\nremark.js\nYou can see an introduction of remark.js from its homepage. You should read the remark.js Wiki at least once to know how to\n\ncreate a new slide (Markdown syntax* and slide properties);\nformat a slide (e.g. text alignment);\nconfigure the slideshow;\nand use the presentation (keyboard shortcuts).\n\nIt is important to be familiar with remark.js before you can understand the options in xaringan.\n.footnote[[*] It is different with Pandoc’s Markdown! It is limited but should be enough for presentation purposes. Come on… You do not need a slide for the Table of Contents! Well, the Markdown support in remark.js may be improved in the future.]\nclass: inverse, middle, center\n\n\nUsing xaringan\n\n\n\nxaringan\nProvides an R Markdown output format xaringan::moon_reader as a wrapper for remark.js, and you can use it in the YAML metadata, e.g.\n---\ntitle: \"A Cool Presentation\"\noutput:\n  xaringan::moon_reader:\n    yolo: true\n    nature:\n      autoplay: 30000\n---\nSee the help page ?xaringan::moon_reader for all possible options that you can use.\n\n\n\nremark.js vs xaringan\nSome differences between using remark.js (left) and using xaringan (right):\n.pull-left[ 1. Start with a boilerplate HTML file;\n\nPlain Markdown;\nWrite JavaScript to autoplay slides;\nManually configure MathJax;\nHighlight code with *;\nEdit Markdown source and refresh browser to see updated slides; ]\n\n.pull-right[ 1. Start with an R Markdown document;\n\nR Markdown (can embed R/other code chunks);\nProvide an option autoplay;\nMathJax just works;*\nHighlight code with {{}};\nThe RStudio addin “Infinite Moon Reader” automatically refreshes slides on changes; ]\n\n.footnote[[*] Not really. See next page.]\n\n\n\nMath Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $+$ renders \\(\\alpha+\\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $.\nMath does not work on the title slide (see #61 for a workaround).\n\n\n\n\nR Code\n\n# a boring regression\nfit = lm(dist ~ 1 + speed, data = cars)\ncoef(summary(fit))\n\n#               Estimate Std. Error   t value     Pr(>|t|)\n# (Intercept) -17.579095  6.7584402 -2.601058 1.231882e-02\n# speed         3.932409  0.4155128  9.463990 1.489836e-12\n\ndojutsu = c('地爆天星', '天照', '加具土命', '神威', '須佐能乎', '無限月読')\ngrep('天', dojutsu, value = TRUE)\n\n# [1] \"地爆天星\" \"天照\"\n\n\n\n\n\nR Plots\n\npar(mar = c(4, 4, 1, .1))\nplot(cars, pch = 19, col = 'darkgray', las = 1)\nabline(fit, lwd = 2)\n\n\n\n\n\n\n\nTables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\nknitr::kable(head(iris), format = 'html')\n\n\n\n \n  \n    Sepal.Length \n    Sepal.Width \n    Petal.Length \n    Petal.Width \n    Species \n  \n \n\n  \n    5.1 \n    3.5 \n    1.4 \n    0.2 \n    setosa \n  \n  \n    4.9 \n    3.0 \n    1.4 \n    0.2 \n    setosa \n  \n  \n    4.7 \n    3.2 \n    1.3 \n    0.2 \n    setosa \n  \n  \n    4.6 \n    3.1 \n    1.5 \n    0.2 \n    setosa \n  \n  \n    5.0 \n    3.6 \n    1.4 \n    0.2 \n    setosa \n  \n  \n    5.4 \n    3.9 \n    1.7 \n    0.4 \n    setosa \n  \n\n\n\n\n\n\n\n\nHTML Widgets\nI have not thoroughly tested HTML widgets against xaringan. Some may work well, and some may not. It is a little tricky.\nSimilarly, the Shiny mode (runtime: shiny) does not work. I might get these issues fixed in the future, but these are not of high priority to me. I never turn my presentation into a Shiny app. When I need to demonstrate more complicated examples, I just launch them separately. It is convenient to share slides with other people when they are plain HTML/JS applications.\nSee the next page for two HTML widgets.\n\n\nlibrary(leaflet)\nleaflet() %>% addTiles() %>% setView(-93.65, 42.0285, zoom = 17)\n\n\n\n\n\n\n\nDT::datatable(\n  head(iris, 10),\n  fillContainer = FALSE, options = list(pageLength = 8)\n)\n\n\n\n\n\n\n\n\n\nSome Tips\n\nDo not forget to try the yolo option of xaringan::moon_reader.\noutput:\n  xaringan::moon_reader:\n    yolo: true\n\n\n\n\nSome Tips\n\nSlides can be automatically played if you set the autoplay option under nature, e.g. go to the next slide every 30 seconds in a lightning talk:\noutput:\n  xaringan::moon_reader:\n    nature:\n      autoplay: 30000\nIf you want to restart the play after it reaches the last slide, you may set the sub-option loop to TRUE, e.g.,\noutput:\n  xaringan::moon_reader:\n    nature:\n      autoplay:\n        interval: 30000\n        loop: true\n\n\n\n\nSome Tips\n\nA countdown timer can be added to every page of the slides using the countdown option under nature, e.g. if you want to spend one minute on every page when you give the talk, you can set:\noutput:\n  xaringan::moon_reader:\n    nature:\n      countdown: 60000\nThen you will see a timer counting down from 01:00, to 00:59, 00:58, … When the time is out, the timer will continue but the time turns red.\n\n\n\n\nSome Tips\n\nThe title slide is created automatically by xaringan, but it is just another remark.js slide added before your other slides.\nThe title slide is set to class: center, middle, inverse, title-slide by default. You can change the classes applied to the title slide with the titleSlideClass option of nature (title-slide is always applied).\noutput:\n  xaringan::moon_reader:\n    nature:\n      titleSlideClass: [top, left, inverse]\n\n–\n\nIf you’d like to create your own title slide, disable xaringan’s title slide with the seal = FALSE option of moon_reader.\noutput:\n  xaringan::moon_reader:\n    seal: false\n\n\n\n\nSome Tips\n\nThere are several ways to build incremental slides. See this presentation for examples.\nThe option highlightLines: true of nature will highlight code lines that start with *, or are wrapped in {{ }}, or have trailing comments #<<;\noutput:\n  xaringan::moon_reader:\n    nature:\n      highlightLines: true\nSee examples on the next page.\n\n\n\n\nSome Tips\n.pull-left[ An example using a leading *:\n```r\nif (TRUE) {\n** message(\"Very important!\")\n}\n```\nOutput:\nif (TRUE) {\n* message(\"Very important!\")\n}\nThis is invalid R code, so it is a plain fenced code block that is not executed. ]\n.pull-right[ An example using {{}}:\n```{r tidy=FALSE}\nif (TRUE) {\n*{{ message(\"Very important!\") }}\n}\n```\nOutput:\n\nif (TRUE) {\n{{ message(\"Very important!\") }}\n}\n\nVery important!\n\n\nIt is valid R code so you can run it. Note that {{}} can wrap an R expression of multiple lines. ]\n\n\n\nSome Tips\nAn example of using the trailing comment #<< to highlight lines:\n```{r tidy=FALSE}\nlibrary(ggplot2)\nggplot(mtcars) + \n  aes(mpg, disp) + \n  geom_point() +   #<<\n  geom_smooth()    #<<\n```\nOutput:\n\nlibrary(ggplot2)\nggplot(mtcars) + \n  aes(mpg, disp) + \n  geom_point() +   #<<\n  geom_smooth()    #<<\n\n\n\n\nSome Tips\nWhen you enable line-highlighting, you can also use the chunk option highlight.output to highlight specific lines of the text output from a code chunk. For example, highlight.output = TRUE means highlighting all lines, and highlight.output = c(1, 3) means highlighting the first and third line.\n```{r, highlight.output=c(1, 3)}\nhead(iris)\n```\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nQuestion: what does highlight.output = c(TRUE, FALSE) mean? (Hint: think about R’s recycling of vectors)\n\n\n\nSome Tips\n\nTo make slides work offline, you need to download a copy of remark.js in advance, because xaringan uses the online version by default (see the help page ?xaringan::moon_reader).\nYou can use xaringan::summon_remark() to download the latest or a specified version of remark.js. By default, it is downloaded to libs/remark-latest.min.js.\nThen change the chakra option in YAML to point to this file, e.g.\noutput:\n  xaringan::moon_reader:\n    chakra: libs/remark-latest.min.js\nIf you used Google fonts in slides (the default theme uses Yanone Kaffeesatz, Droid Serif, and Source Code Pro), they won’t work offline unless you download or install them locally. The Heroku app google-webfonts-helper can help you download fonts and generate the necessary CSS.\n\n\n\n\nMacros\n\nremark.js allows users to define custom macros (JS functions) that can be applied to Markdown text using the syntax ![:macroName arg1, arg2, ...] or ![:macroName arg1, arg2, ...](this). For example, before remark.js initializes the slides, you can define a macro named scale:\nremark.macros.scale = function (percentage) {\n  var url = this;\n  return '<img src=\"' + url + '\" style=\"width: ' + percentage + '\" />';\n};\nThen the Markdown text\n![:scale 50%](image.jpg)\nwill be translated to\n<img src=\"image.jpg\" style=\"width: 50%\" />\n\n\n\n\nMacros (continued)\n\nTo insert macros in xaringan slides, you can use the option beforeInit under the option nature, e.g.,\noutput:\n  xaringan::moon_reader:\n    nature:\n      beforeInit: \"macros.js\"\nYou save your remark.js macros in the file macros.js.\nThe beforeInit option can be used to insert arbitrary JS code before remark.create(). Inserting macros is just one of its possible applications.\n\n\n\n\nCSS\nAmong all options in xaringan::moon_reader, the most challenging but perhaps also the most rewarding one is css, because it allows you to customize the appearance of your slides using any CSS rules or hacks you know.\nYou can see the default CSS file here. You can completely replace it with your own CSS files, or define new rules to override the default. See the help page ?xaringan::moon_reader for more information.\n\n\n\nCSS\nFor example, suppose you want to change the font for code from the default “Source Code Pro” to “Ubuntu Mono”. You can create a CSS file named, say, ubuntu-mono.css:\n@import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);\n\n.remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }\nThen set the css option in the YAML metadata:\noutput:\n  xaringan::moon_reader:\n    css: [\"default\", \"ubuntu-mono.css\"]\nHere I assume ubuntu-mono.css is under the same directory as your Rmd.\nSee yihui/xaringan#83 for an example of using the Fira Code font, which supports ligatures in program code.\n\n\n\nCSS (with Sass)\nxaringan also supports Sass support via rmarkdown. Suppose you want to use the same color for different elements, e.g., first heading and bold text. You can create a .scss file, say mytheme.scss, using the sass syntax with variables:\n$mycolor: #ff0000; \n.remark-slide-content > h1 { color: $mycolor; }\n.remark-slide-content strong { color: $mycolor; }\nThen set the css option in the YAML metadata using this file placed under the same directory as your Rmd:\noutput:\n  xaringan::moon_reader:\n    css: [\"default\", \"mytheme.scss\"]\nThis requires rmarkdown >= 2.8 and the sass package. You can learn more about rmarkdown and sass support in this blog post and in sass overview vignette.\n\n\n\nThemes\nDon’t want to learn CSS? Okay, you can use some user-contributed themes. A theme typically consists of two CSS files foo.css and foo-fonts.css, where foo is the theme name. Below are some existing themes:\n\nnames(xaringan:::list_css())\n\n [1] \"chocolate-fonts\"  \"chocolate\"        \"default-fonts\"   \n [4] \"default\"          \"duke-blue\"        \"fc-fonts\"        \n [7] \"fc\"               \"glasgow_template\" \"hygge-duke\"      \n[10] \"hygge\"            \"ki-fonts\"         \"ki\"              \n[13] \"kunoichi\"         \"lucy-fonts\"       \"lucy\"            \n[16] \"metropolis-fonts\" \"metropolis\"       \"middlebury-fonts\"\n[19] \"middlebury\"       \"nhsr-fonts\"       \"nhsr\"            \n[22] \"ninjutsu\"         \"rladies-fonts\"    \"rladies\"         \n[25] \"robot-fonts\"      \"robot\"            \"rutgers-fonts\"   \n[28] \"rutgers\"          \"shinobi\"          \"tamu-fonts\"      \n[31] \"tamu\"             \"uio-fonts\"        \"uio\"             \n[34] \"uo-fonts\"         \"uo\"               \"uol-fonts\"       \n[37] \"uol\"              \"useR-fonts\"       \"useR\"            \n[40] \"uwm-fonts\"        \"uwm\"              \"wic-fonts\"       \n[43] \"wic\"             \n\n\n\n\n\nThemes\nTo use a theme, you can specify the css option as an array of CSS filenames (without the .css extensions), e.g.,\noutput:\n  xaringan::moon_reader:\n    css: [default, metropolis, metropolis-fonts]\nIf you want to contribute a theme to xaringan, please read this blog post.\nbackground-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg) background-size: 100px background-position: 90% 8%\n\n\nSharingan\nThe R package name xaringan was derived1 from Sharingan, a dōjutsu in the Japanese anime Naruto with two abilities:\n\nthe “Eye of Insight”\nthe “Eye of Hypnotism”\n\nI think a presentation is basically a way to communicate insights to the audience, and a great presentation may even “hypnotize” the audience.2,3\n.footnote[ [1] In Chinese, the pronounciation of X is Sh /ʃ/ (as in shrimp). Now you should have a better idea of how to pronounce my last name Xie.\n[2] By comparison, bad presentations only put the audience to sleep.\n[3] Personally I find that setting background images for slides is a killer feature of remark.js. It is an effective way to bring visual impact into your presentations. ]\n\n\n\nNaruto terminology\nThe xaringan package borrowed a few terms from Naruto, such as\n\nSharingan (写輪眼; the package name)\nThe moon reader (月読; an attractive R Markdown output format)\nChakra (查克拉; the path to the remark.js library, which is the power to drive the presentation)\nNature transformation (性質変化; transform the chakra by setting different options)\nThe infinite moon reader (無限月読; start a local web server to continuously serve your slides)\nThe summoning technique (download remark.js from the web)\n\nYou can click the links to know more about them if you want. The jutsu “Moon Reader” may seem a little evil, but that does not mean your slides are evil.\n\nclass: center\n\n\nHand seals (印)\nPress h or ? to see the possible ninjutsu you can use in remark.js.\n\n\nclass: center, middle\n\n\nThanks!\nSlides created via the R package xaringan.\nThe chakra comes from remark.js, knitr, and R Markdown."
  },
  {
    "objectID": "week2.html#additional-reading",
    "href": "week2.html#additional-reading",
    "title": "2  SAR Sensor",
    "section": "2.3 Additional Reading",
    "text": "2.3 Additional Reading\nRADAR VS. OPTICAL: OPTIMISING SATELLITE USE IN LAND COVER CLASSIFICATION https://ecologyforthemasses.com/2020/05/27/radar-vs-optical-optimising-satellite-use-in-land-cover-classification/\n An optical image of Kliuchevskoi volcano on the left, with a radar image on the right (Image credit: Michigan Tech Volcanology, Image Cropped)\n\nComparison of land cover classification results using a temporal average (left) and time series (right) of a combination of optical and radar images (Image Credit: Lopes et al., 2020)\nThe categorization accuracy was tested using the so-called Kappa-coefficient. According to their findings, classifications based on radar time series were more accurate than those based on optical time series. However, more precise findings may not always result in better maps. Therefore, while examining maps, the possibility of inaccuracies caused by the similarity of categories in optical or radar data should be considered."
  },
  {
    "objectID": "week2.html#radar-vs.-optical-optimising-satellite-use-in-land-cover-classification",
    "href": "week2.html#radar-vs.-optical-optimising-satellite-use-in-land-cover-classification",
    "title": "2  SAR Sensor",
    "section": "2.2 Radar VS. Optical: Optimising satellite use in land cover classification",
    "text": "2.2 Radar VS. Optical: Optimising satellite use in land cover classification\nhttps://ecologyforthemasses.com/2020/05/27/radar-vs-optical-optimising-satellite-use-in-land-cover-classification/\n An optical image of Kliuchevskoi volcano on the left, with a radar image on the right (Image credit: Michigan Tech Volcanology, Image Cropped)\n\nComparison of land cover classification results using a temporal average (left) and time series (right) of a combination of optical and radar images (Image Credit: Lopes et al., 2020)\nThe categorization accuracy was tested using the so-called Kappa-coefficient. According to their findings, classifications based on radar time series were more accurate than those based on optical time series. However, more precise findings may not always result in better maps. Therefore, while examining maps, the possibility of inaccuracies caused by the similarity of categories in optical or radar data should be considered."
  },
  {
    "objectID": "week2.html#synthetic-aperture-radar-sar",
    "href": "week2.html#synthetic-aperture-radar-sar",
    "title": "2  SAR Sensor",
    "section": "2.1 Synthetic Aperture Radar (SAR)",
    "text": "2.1 Synthetic Aperture Radar (SAR)\n\nUsing radar over optical remote sensing\nAdvantanges\n\n\n(+) Ability to capture data day or night\n\n\n(+) See through clouds\n\n\n(+) Weather independence by selecting proper frequency range\n\n\n(+) Penetration through the vegetation canopy and the soil\n\n\n(+/-) Sensitivity to structure\nDisadvantanges\n\n\n(-) Information content is different than optical and sometimes difficult to interpret\n\n\nVariable resolution is 1 to 100 m"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "https://ecologyforthemasses.com/2020/05/27/radar-vs-optical-optimising-satellite-use-in-land-cover-classification/\nhttps://www.l3harrisgeospatial.com/Learn/Blogs/Blog-Details/ArtMID/10198/ArticleID/24031/Enhancing-Situational-Awareness-with-SAR\nhttps://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar"
  },
  {
    "objectID": "week1.html#applications",
    "href": "week1.html#applications",
    "title": "1  An Introduction to Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nThere are many useful applications of remote sensing. Examples of agricultural applications include productivity increasing. This could be done by providing inputs, given from remote sensing data, for improvement of soil fertility. The other example is to identify culturable wastelands or marginal lands. The benefit of this application is to increase agricultural area (Navalgund, Jayaraman, and Roy 2007). Without remote sensing data, it will be difficult to retrieve data covering every area in the country."
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "1  An Introduction to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nAs I used to work in the feed trading company before, remote sensing plays an important role of estimating crop supply. To identify the location of all crops such as corn and soybean in the country, we need to depend on remote sensing data to quickly estimate the supply of each crop. Otherwise, human are needed to survey in every fields, which is quite exhaustive. Furthermore, it is good to see the availability of data with different resolution, so we can choose and use it with a proper task.\n\n\n\n\nNavalgund, Ranganath R., V. Jayaraman, and P. S. Roy. 2007. “Remote Sensing Applications: An Overview.” Current Science 93 (12): 1747–66. http://www.jstor.org/stable/24102069."
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "3  Remote sensing data",
    "section": "",
    "text": "Remote Sensing data are usually not ready to use, due to many reasons such as errors from sensor devices and cloud covering. Therefore, it is needed to correct before using. There are many corrections techniques as following:\n\n\n\n\n\n\n\nCorrection\nSolutions\n\n\n\n\n1. Geometric correction\nCorrection Techniques: Backward mapping (Linear Regression), Resampling  Explanation: Remotely sensed image distortions can be happened due to view angle, topography or wind. Therefore, we may use GCP (Ground control points) with polynomial function to correct\n\n\n2. Atmospheric Correction\nCorrection Techniques: Dark object subtraction (DOS), Psuedo-invariant Features (PIFs) Explanation: Absorption and scattering may reduces contrast of image and cause adjacency effect, which is the situation when radiance from pixels nearby is mixed into pixel of interest. One of the solutions is to apply Dark object subtraction (DOS) by searching each band for the darkest value then subtracts that from each pixel. The other method is Psuedo-invariant Features (PIFs), which is to adjust the image based on the regression result.\n\n\n3. Orthorectification correction\nCorrection Techniques: Cosine correction Explanation: According to the image, the street looks curve, even if the reality is the straight street. Therefore, cosine correction can be applied to remove distortions and making the pixels viewed at nadir (straight down).\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nDigital number (DN)\nIntensity of the electromagnetic radiation per pixel\n\n\nRadiance\nHow much light the instrument sees in meaningful units\n\n\nReflectance\nthe ratio of light leaving the target to amount striking the target.\n\n\nTOA reflectance\nMeasure radiation going down and up in a ring\n\n\nSurface reflectance\nMeasure radiation going down\n\n\nhemispherical reflectance\nAll of the light leaving the surface goes straight to the sensor"
  },
  {
    "objectID": "week3.html#applications",
    "href": "week3.html#applications",
    "title": "3  Remote sensing data",
    "section": "3.2 Applications",
    "text": "3.2 Applications"
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "3  Remote sensing data",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection"
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "4  Policy Applications",
    "section": "",
    "text": "This week, we will give an example of urban policies and how to incorporate remotely sensed data to achieve their plans."
  },
  {
    "objectID": "week4.html#applications",
    "href": "week4.html#applications",
    "title": "4  Policy",
    "section": "4.2 Applications",
    "text": "4.2 Applications"
  },
  {
    "objectID": "week4.html#reflection",
    "href": "week4.html#reflection",
    "title": "4  Policy Applications",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nThis week’s lessons have been helpful in understanding the larger process of urban plans and improvement, of which remote sensing is an essential part. There are also global issues that need to be addressed. However, one solution may not fit all, due to the difference between areas. This lesson also shows how each local government address the similar issues in different ways. Ultimately, I could see the concrete action plans to address each urban issue from New York City’s strategic plan, which could be adopted to partially used for addressing similar issues in my country too.\n\n\n\n\nRolnick, David, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, et al. 2022. “Tackling Climate Change with Machine Learning.” ACM Computing Surveys 55 (2): 42:1–96. https://doi.org/10.1145/3485128.\n\n\nUS EPA, OAR. 2015. “Sources of Greenhouse Gas Emissions.” Overviews and {Factsheets}. https://www.epa.gov/ghgemissions/sources-greenhouse-gas-emissions."
  },
  {
    "objectID": "week4.html#summary",
    "href": "week4.html#summary",
    "title": "4  Policy",
    "section": "4.1 Summary",
    "text": "4.1 Summary\n\nPolicy: OneNYC 2050, New York\n\n\nSelected policies\n\n\nVolume 7 of 9 - A Livable Climate Initiative 20. Achieve Carbon Neutrality And 100 Percent Clean Electricity By 2050, New York City will have net-zero GHG emissions citywide. To achieve this, we will reduce our emissions as much as possible and offset our “irreducible emissions,” GHG emissions by Sector - Building, Transportation, Waste\nVolume 8 of 9 - Efficient Mobility Initiative 26. Reduce congestion and emissions GHG emissions reduction - be able to reduce greenhouse gas (GHG) emissions, support sustainable growth, and achieve the ambitious goal we set in 2015 to have 80 percent of all trips in the city taken by sustainable modes by 2050 Green The fleet - Near term emissions reductions will be achieved by implementing renewable diesel fuel, accelerating the transition to EV and hybrid vehicles, and increasing the efficiency of the fleet, which will help reduce the City’s fuel consumption to below 2014 levels."
  },
  {
    "objectID": "week4.html#policies-summary",
    "href": "week4.html#policies-summary",
    "title": "4  Policy Applications",
    "section": "4.1 Policies Summary",
    "text": "4.1 Policies Summary\n\nPolicy: OneNYC 2050, New York\n\n\nSelected policies\n\n\nVolume 7 of 9 - A Livable Climate Initiative 20: “Achieve Carbon Neutrality And 100 Percent Clean Electricity”\n\nGoal: By 2050, New York City will have net-zero greenhouse gas (GHG) emissions citywide. To achieve this, we will reduce our emissions as much as possible and offset our “irreducible emissions”\nAs transportation is the main cause of greenhouse gas emission in the United States (US EPA 2015), we will further look on the policies related to transportation for further investigating how can we amend the situation of greenhouse gas caused by transports.\n\n\n\n\n\n\n\n\nSource: US EPA, OAR. 2015\n\n\nVolume 8 of 9 - Efficient Mobility Initiative 26: Reduce congestion and emissions\n\nGoal: GHG emissions reduction - be able to reduce greenhouse gas (GHG) emissions, support sustainable growth, and achieve the ambitious goal we set in 2015 to have 80 percent of all trips in the city taken by sustainable modes by 2050\nGreen The fleet - Near term emissions reductions will be achieved by implementing renewable diesel fuel, accelerating the transition to EV and hybrid vehicles, and increasing the efficiency of the fleet, which will help reduce the City’s fuel consumption to below 2014 levels."
  },
  {
    "objectID": "week4.html#applications-using-remote-sensing",
    "href": "week4.html#applications-using-remote-sensing",
    "title": "4  Policy Applications",
    "section": "4.2 Applications using Remote Sensing",
    "text": "4.2 Applications using Remote Sensing\n\n\n\n\n\n\nSource: Rolnick 2022\n\nThere are many opportunities to reduce GHG emissions from transportation using machine learning and remote sensing. (Rolnick et al. 2022)\nIn our case study, we aim to incorporate remote sensing to reduce transportation activity and track the use of EV and hybrid vehicle which will result in CO2 emission reduction. Our proposed methodology is to monitor CO2 and transport mode in each area, then we can summarise the statistics of transports used in each area along with CO2 emission in each area too. According to our goal to make the transition to EV and hybrid vehicles, this information will benefit policymaker to track the progress of transition and CO2 emission reduction in each area.\nGoal: Predict change in CO2 from switching behaviour in transport mode for decarbonizing transportation\nSubtask:\n\nestimate average vehicle traffic\nclassify vehicle types\nmonitor CO2 emission\n\nData:\n\nRemote Sensing data\n\nWorldwide but lower resolution e.g. Sentinel-1/2, Landsat\nHigh resolution e.g. Vehicle Detection in Aerial Imagery (VEDAI)\n\nvehicle label for detection and classification\nCO2 Detected from Satellite data, e.g. Orbiting Carbon Observatory-3 (OCO-3)\n\nProposed Solutions:\n\nTask1: estimate average vehicle traffic\n\nIssue: Traditionally, traffic is monitored with ground based counters that are installed on selected roads. As ground-based counters require costly installation and maintenance, many countries do not have such systems.\nMethodology: Vehicles can also be detected in high-resolution satellite images with high accuracy, and image counts can serve to estimate average vehicle traffic\nData: Remote Sensing data and vehicle label for detection\n\nTask2: classify vehicle types\n\nMethodology: Use convolutional neural network (CNN) with satellite imagery to classify vehicle types\nData: Remote Sensing data and vehicle label for classification\nSample project: https://github.com/AlperenCicek/vehicle-detection-from-satellite\n\nTask3: monitor CO2 emission\n\nMethodology: Having real-time maps of GHGs (detected from remote sensing) could help us quantify CO2 emissions. For example, data on emissions make it possible to set effective targets, and pinpointing the sources of emissions makes it possible to enforce regulation\nData: Orbiting Carbon Observatory-3 (OCO-3) - measures and maps column CO2 in great detail\n\n\nGlobal agendas:\nAccording to United Nations 2030 Sustainable Development Goals (SDGs) agenda, our approach could help enable 2 SDGs which are\n\nSDG 11: Sustainable Cities and Communities\n\nreduce greenhouse gas emissions - a quarter of energy-related global greenhouse gas emissions come from transport and that these emissions are projected to grow substantially in the years to come\n\nSDG 13: Take urgent action to combat climate change and its impacts\n\nreaching net-zero carbon dioxide CO2 emissions globally by 2050"
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "5  Introduction to Google Earth Engine",
    "section": "",
    "text": "This weeks, we will talk about one of the most powerful platforms to implement remote sensing applications, which is Google Earth Engine.\n\n\n\n\n\n\nSource: Google Earth Engine\n\n\n\nGoogle Earth Engine (GEE) is a platform for the cloud-based processing of scientific data. The key feature that attracts the majority of GIS users is its ability to perform large computations extremely quickly, as most remote sensing data are quite huge and it is difficult to conduct research on a very broad region. Moreover, it also provides the visualisation panel that we can run the code and see the output from the map in a minute. The coding language used in the platform is javascript. However, they also provide Google Earth Engine Python API for users who get familiar with python.\nThe interface of Google Earth Engine platform is shown below:\n\n\n\n\n\n\nSource: GEE community Beginner’s Cookbook, TC25\n\n\n\n\nEarth engine has its own data types or terms, which are explained as follow:\n\n\n\n\n\n\nSource: GEE\n\n\nData types in Google Earth Engine\n\nImage = Raster\nFeature = Vector\nImageCollection = Image stack\nFeatureCollection = Feature stack\n\n\n\n\n\nThe programming language used in GEE is javascript. We can use any basic functions just like what we used to do. Some sets of functions are run on the server side, such as ee.ImageCollection([PATH]), so we can tackle with very large datasets. However, there is a few warnings, which should be aware of. One thing that should be remembered is looping. Don’t use looping function on the server, since it does not allow earth engine to run distributed for a quick process. The alternative function of looping is map().\nExample of using map() to capitalise instead of for() for looping:\n\n# map\nnames.map(name => capitalize(name))\n\nMoreover, there are many geometry operations to help process data, including joins, zonal statistics (e.g. average temperature per neighbourhood), filtering of images or specific values.\n\nWe can both upload our own datasets, which can be shapefile, into GEE assets and use them or select data from Earth Engine data catalog.\nThe example of loading data, filtering specific periods and bounding areas is provided below:\n\nvar india = ee.FeatureCollection('projects/ucl-remote-sensing/assets/india_boundary')\n    .filter('GID_1 == \"IND.25_1\"');\n\nvar oneimage_study_area_cloud = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n  .filterDate('2021-06-01', '2022-10-10')\n  .filterBounds(india)  // Intersecting ROI\n  .filter(ee.Filter.lt(\"CLOUD_COVER\", 0.1));\n\n\nThere are many functions for processing raster and vector data provided in GEE. We will show examples of some functions that are usually used in the preprocessing steps.\n\nMosaic images: taken images according to their order in the collection (last on top)\n\n\nvar mosaic = oneimage_study_area_cloud_scale.mosaic();\n\nvar vis_params2 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0.0,\n  max: 0.3,\n};\n\nMap.addLayer(mosaic, vis_params2, 'spatial mosaic');\n\n\nClip images: filter images from a given areas specified in feature/shapefile/vector.\n\n\nvar clip = meanImage.clip(india)\n  .select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7']);\n\nvar vis_params3 = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0,\n  max: 0.3,\n};\n\n// map the layer\nMap.addLayer(clip, vis_params3, 'clip');\n\n\nCalculate statistics: such as median\n\n\nvar median = oneimage_study_area_cloud.reduce(ee.Reducer.median());"
  },
  {
    "objectID": "week5.html#applications",
    "href": "week5.html#applications",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nAfter we have finished conducting our own analysis, we can also publish our analysis via Google Earth Engine Apps, which provide ability to turn code into responsive applications. Some great examples are selected to present below:\n\n1. Europe’s Air Quality Winner (Source: gaertnerp)\n\n\n\n\n\n\nThis application was built to visualise the improvement of air quality in Europe after the lockdown due to the corona pandemic. The amount of NO2 was captured by remote sensing and shown in chrolopleth map. Users can compare the situation between 2 periods and The difference of NO2 median composite was calculated and shown. In my perspective, it is quite great visualisation that users can understand how much the air quality changes in a minute. However, there is some limitation of the visualisation that it is not allowed to zoom-in to see more details in each specific area.\n\n2. NDVI Slider (Source: khaledalshamaa)\n\n\n\n\n\n\nThe developer aims to compare Normalized Difference Vegetation Index (NDVI) between 2 years. The source of data is MODIS, which has the spatial resolution of 250 m and temporal resolution of 16-day. In this work, it is better than the previous work above in aspect that users can zoom-in to see NDVI in specific area."
  },
  {
    "objectID": "week5.html#reflection",
    "href": "week5.html#reflection",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\n\nTools: I found that Google Earth Engine is a very powerful platform since it provides server side functions and large servers for high computation which is best fit with remote sensing projects that usually require big data for conducting researches. Moreover, it also provides UI for visualising maps from our codes. Before knowing the available of GEE, I found that it is quite hard to conduct remote sensing products since it took a very long time to conduct a research from data collection to data processing. Moreover, Earth Engine Data Catalog is very useful for the use of data. It provides many common remote sensing data that can be used for analysis in many tasks.\nApplications: Earth Engine App is very convenient for publishing analysis to let other people to see our results. It is also good for exploring data and finding insight. The great part of it is the interactive part which allow users to adjust parameters and see the results."
  }
]